{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeljov/NAP2025/blob/main/Pride_and_Prejudice_network_analysis.ipynb)"
  },
  {
   "metadata": {
    "id": "md_qRVx2QtJm"
   },
   "cell_type": "markdown",
   "source": [
    "## Text analytics and SNA applied to Jane Austen's \"Pride and Prejudice\" novel\n",
    "\n",
    "Pride and Prejudice is one of the most beloved English classic novels from the first half of the 19th century. As is the case with many other classics, the novel is made publicly available as a part of the [Project Gutenberg](https://www.gutenberg.org/).\n",
    "\n",
    "Similar to how movie scripts were used at [Movie Galaxies](https://moviegalaxies.com/) to build networks of movie characters, we will use the (publicly available) text of Pride and Prejudice to create a social network of its characters. In the case of networks of movie character, two characters are connected if they occur in the same scene. Analogous to that, we will create a network, where two characters are connected if they occur in the same paragraph, and the frequncy of their co-occurrence will be reflected in the weight of the edge that connects them.\n",
    "\n",
    "To build such a network, we will adopt the following method:\n",
    "* **Download and preprocess the book content**. Download the book as a .txt file either directly from [Project Gutenberg](https://www.gutenberg.org/ebooks/1342) of from one of many public repos where it is available. Then, split the book into volumes and chapters, and do some text cleaning along the way.\n",
    "* **Collect data about the book characters**. Look for a place on the net where the list of book characters can be found - this will be needed to properly identify characters in the text. For example, one such list is available [here](https://austenprose.com/pride-and-prejudice-character-list/), as a part of website that is generally about Jane Austen's work and can be considered as a credible source. Extract the list of characters and make it available for the entity extraction task.\n",
    "* **Do paragraph-level enitity (character) extraction**. Extract entities from each book paragraph using spaCy for tokenisation and entity detection. Since spaCy's NER (Named Entity Recognition) module was trained on news arcticle and web content, it is not able to handle the text of a 19th century novel properly. Thus, we need to define a set of custom rules for the entity detection task.\n",
    "* **Create an edge list**. Recall that a typical source for network creation is an edge list. So, having identified entities in each paragraph, for each paragraph with at least two entities detected, we add all pairs of the identified entities to an edge list. For example, if Elizabeth, Jane, and Darcy are mentioned in one paragraph, we would add three edges to the edge list: Elizabeth - Jane, Elizabeth - Darcy, and Jane - Darcy. After all edges (i.e., detected entity pairs) have been added to the edge list, reduce the list by introducing edge weight, so that instead of having multiple occurrences of the same edge, we have one edge occurrence with the weight reflecting its frequency.\n",
    "* **Create and visualise the (paragraph-level) network**. The edge list can be directly used to create a network using the networkX library. Then we visualise and explore the network.\n",
    "* **Explore the network**. Compute the basic netowrk statistics and identify the most central characters. Examine how connected the networkk is and if some communities can be detected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instal and load the required libraries"
   ],
   "metadata": {
    "id": "q0vhgK5_nzoX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# pyvis is a network visualisation library capable of much better network visualisation compared to that provided by networkX\n",
    "# https://pypi.org/project/pyvis/\n",
    "\n",
    "!pip install -q pyvis"
   ],
   "metadata": {
    "id": "r9NbbK_Qi6R1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yhqd-U8ci_VB"
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from google.colab import files\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from pyvis.network import Network\n",
    "import IPython\n",
    "\n",
    "import warnings"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download and preprocess the book content"
   ],
   "metadata": {
    "id": "KFwsdkYYn8GF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The book content was downloaded from [this GitHub repo](https://github.com/laumann/ds/blob/master/hashing/books/) and stored localy."
   ],
   "metadata": {
    "id": "PaAWfNIAVS5m"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "book_file = files.upload()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "INS3bPGPPFKx",
    "outputId": "afd43d9b-411e-40fc-8a64-68065aec6e71"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "with open('jane-austen-pride-prejudice.txt', 'r') as fobj:\n",
    "  book_raw_txt = fobj.read()\n",
    "\n",
    "print(len(book_raw_txt))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gGZ9KQvEPbln",
    "outputId": "3f16903b-bbb8-475d-97a4-edc28aba7eb2"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split the book content into volumes, chapters, and eventually paragrpahs"
   ],
   "metadata": {
    "id": "5krhQ3jWPtqc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Regarding the regex patterns below:\n",
    "# \\s+ matches one or more whitespace characters\n",
    "# [IVXLCDM]+ matches one or more Roman numeral characters\n",
    "# \\. captures the dot at the end of the chapter number\n",
    "volume_pattern = r'(VOL\\.\\s+[IVXLCDM]+\\.)'\n",
    "chapter_pattern = r'(CHAPTER\\s+[IVXLCDM]+\\.)'\n",
    "\n",
    "# split the text into volumes\n",
    "# note that we included parentheses in the pattern to keep the volume titles in the text\n",
    "book_volumes = re.split(volume_pattern, book_raw_txt)\n",
    "\n",
    "# the text of VOL. I is preceded by the 'preface'\n",
    "preamble = book_volumes[0]\n",
    "\n",
    "chapters = list()\n",
    "\n",
    "for i in range(1, len(book_volumes)-1, 2):\n",
    "    volume_lbl = book_volumes[i].strip()\n",
    "    volume_content = book_volumes[i+1].strip()\n",
    "\n",
    "    # split a volume into chapters\n",
    "    volume_chapters = re.split(chapter_pattern, volume_content)\n",
    "    # skip the content before the first chapter, it's a preface\n",
    "    for i in range(1, len(volume_chapters)-1, 2):\n",
    "      chapter_lbl = volume_chapters[i].strip()\n",
    "      chapter_content = volume_chapters[i+1].strip().strip(\"\\n\")\n",
    "      chapters.append({\n",
    "          'volume':volume_lbl,\n",
    "          'chapter':chapter_lbl,\n",
    "          'content':chapter_content\n",
    "      })\n",
    "\n"
   ],
   "metadata": {
    "id": "lN1IlFPjPxFO"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "len(chapters)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyDFRx3TRWM2",
    "outputId": "0eb55ef8-4635-48c0-a28f-38ee408ec705"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(chapters[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vdANFBfgUzys",
    "outputId": "37846dc7-d2d5-4544-e280-1a25cdfab8d6"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Collect data about the book characters\n",
    "\n",
    "The character data will be collected by scraping the relevant content of the [Pride and Prejudice: List of Characters](https://austenprose.com/pride-and-prejudice-character-list/) page."
   ],
   "metadata": {
    "id": "4jwfzoH-rxCL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "char_groups = ['Longbourn', 'Netherfield Park', 'Lucas Lodge', 'Meryton', 'Rosings Park', 'Pemberley', 'Town', 'Regiment']"
   ],
   "metadata": {
    "id": "gMSuzZ3bSBkr"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "characters = defaultdict(list)\n",
    "url = \"https://austenprose.com/pride-and-prejudice-character-list/\"\n",
    "\n",
    "try:\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    main_chars_elem = soup.find(lambda elem: (elem.name=='div') and elem.has_attr('class') and (elem['class'][0] == 'post-content'))\n",
    "\n",
    "    current_char_group = \"\"\n",
    "    for elem in main_chars_elem.find_all(lambda elem: elem.name in ['h3', 'p']):\n",
    "\n",
    "      if (elem.name == 'h3'):\n",
    "        if elem.text.strip() == \"Minor Characters\":\n",
    "          break\n",
    "        group_lbl = elem.text.split(maxsplit=1)[1].strip()\n",
    "        if (group_lbl in char_groups) and (current_char_group != group_lbl):\n",
    "          current_char_group = group_lbl\n",
    "\n",
    "      if (current_char_group != \"\") and (elem.name == 'p') and (elem.find_next(name='strong')):\n",
    "          char_in_bold = elem.find_next('strong')\n",
    "          characters[current_char_group].append({'name': char_in_bold.text, 'description':elem.text})\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(e.with_traceback)"
   ],
   "metadata": {
    "id": "PkwdprsOjH76"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "characters_list = list()\n",
    "for char_group, char_list in characters.items():\n",
    "  for char in char_list:\n",
    "    char.update({'group':char_group})\n",
    "    characters_list.append(char)"
   ],
   "metadata": {
    "id": "dbiRQBBbBSG9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "len(characters_list)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ocP8rz7EGJzO",
    "outputId": "7f0a370e-499e-4621-95ee-c2da073b90fd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for char in characters_list[:5]:\n",
    "  print(char)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3NZgvJ3C4lN",
    "outputId": "d703de0e-b14c-4861-f11b-434f1573f568"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "characters_df = pd.DataFrame(characters_list)\n",
    "\n",
    "print(characters_df.shape)\n",
    "characters_df.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "VUOjQca9l1rG",
    "outputId": "ae027163-22ea-4477-8d50-baef785e47ed"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "characters_df.tail()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "7SOmZ1WsDwzR",
    "outputId": "1e8ceeb6-8b23-48ff-d23b-4a0b1b3729fb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "characters_df.name.tolist()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9JFqGlvEpz9",
    "outputId": "0b74f0af-84ad-4d52-86df-bb84bce11aa9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since each character can be referenced in the text in many different ways, we will first split the full name into segments (title, first name, last name) in order to be able to create various versions of character mentions in the text."
   ],
   "metadata": {
    "id": "8BvgDJsKtz3T"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_title_firstname_surname(full_name):\n",
    "  name_parts = full_name.split()\n",
    "  if len(name_parts) == 2:\n",
    "    title, surname = name_parts\n",
    "    return title, None, surname\n",
    "  elif len(name_parts) == 3:\n",
    "    title, fname, surname = name_parts\n",
    "    return title, fname, surname\n",
    "  else:\n",
    "    surname = \" \".join(name_parts[-2:])\n",
    "    fname = name_parts[-3]\n",
    "    n = len(name_parts)\n",
    "    title = \" \".join(name_parts[0:(n-3)])\n",
    "    return title, fname, surname"
   ],
   "metadata": {
    "id": "nHQnpg3sH-i3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "name_parts_df = characters_df.name.apply(extract_title_firstname_surname).apply(pd.Series)\n",
    "name_parts_df.columns = ['title', 'first_name', 'last_name']\n",
    "\n",
    "pride_prej_chars_df = pd.concat([name_parts_df, characters_df], axis=1)\n",
    "pride_prej_chars_df"
   ],
   "metadata": {
    "id": "h3hrXVywG5Pc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "5de141cd-435f-47cd-d07f-522682b244c4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now add a column with all possible name variants to look for in the text:\n",
    "* full name,\n",
    "* title + first name (if first name exists)\n",
    "* title + surname (if first name exists),\n",
    "* first name only (if it exists),\n",
    "* special cases:\n",
    "  * include nicknames \"Lizzy\" and \"Eliza\" for Elizabeth, as well as \"Mrs. Darcy\" (what she becomes at the very end),\n",
    "  * include nickname \"Kitty\" for Catherine Bennet\n",
    "  * include surnames (without the title) for Bingly and Darcy, as they are often referenced like that in the book\n",
    "  * Darcy is never referred to by his first name and it might be better not included not to mix him with Colonel Fitzwilliam, his cousin.\n",
    "  * exclude the title + surname combination for the Bennet sisters as for them this combination is the same and thus introduces ambiguity\n",
    "  * include \"Lady Catherine\" for Rt. Hon. Lady Catherine de Bourgh\n",
    "  * include labels 'the late Mr. Darcy', 'the elder Mr. Darcy', and \"old Mr. Darcy\", for Mr. Darcy's father; this is to handle the ambiguity that tends to arise from both the father and the son being referred to \"Mr. Darcy\"."
   ],
   "metadata": {
    "id": "M8Sn3MANINkE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "chars_name_variants = []\n",
    "\n",
    "for _, row in pride_prej_chars_df.iterrows():\n",
    "  fn = row['first_name']\n",
    "  ln = row['last_name']\n",
    "  title = row['title']\n",
    "\n",
    "  name_variants = [row['name']]\n",
    "\n",
    "  if fn:\n",
    "    name_variants.extend([f\"{title} {fn}\", f\"{fn} {ln}\"])\n",
    "\n",
    "  # the following one is to add the title + surname combination for all with the first name\n",
    "  # except for the Bennet sisters - see the special rules above\n",
    "  if fn and (ln != \"Bennet\"):\n",
    "    name_variants.append(f\"{title} {ln}\")\n",
    "\n",
    "  # the following one is to add the first name for all characters except Darcy - see the special rules above\n",
    "  if fn and (ln != \"Darcy\"):\n",
    "    name_variants.append(fn)\n",
    "\n",
    "  if row['name'] == 'Miss Elizabeth Bennet':\n",
    "    name_variants.extend(['Lizzy', 'Eliza', 'Miss Eliza', 'Mrs. Darcy'])\n",
    "\n",
    "  if row['name'] == 'Miss Catherine Bennet':\n",
    "    name_variants.extend(['Kitty', 'Miss Kitty'])\n",
    "\n",
    "  if row['name'] in ['Mr. Charles Bingley', 'Mr. Fitzwilliam Darcy']:\n",
    "    name_variants.append(ln)\n",
    "\n",
    "  if row['name'] == 'Mr. Darcy (the elder)':\n",
    "    name_variants = ['the late Mr. Darcy', 'the elder Mr. Darcy']\n",
    "\n",
    "  chars_name_variants.append({\n",
    "      'name': row['name'],\n",
    "      'name_variants': name_variants\n",
    "  })\n",
    "\n",
    "chars_name_variants_df = pd.DataFrame(chars_name_variants)\n",
    "#chars_name_variants_df"
   ],
   "metadata": {
    "id": "lbIQM3l7KicW"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "pride_prej_chars_df = pd.merge(pride_prej_chars_df, chars_name_variants_df, on='name', how='inner')\n",
    "pride_prej_chars_df.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWDTU3hjORzd",
    "outputId": "d0e1fcb8-50b0-4587-b932-a6742f69fe7c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "pride_prej_chars_df.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "tbMNTKI6Okxr",
    "outputId": "1b25988b-7142-4d93-db17-4dd0b9767993"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "pride_prej_chars_df.to_csv(\"pride_and_prejudice_characters.csv\", index=False)\n",
    "files.download(\"pride_and_prejudice_characters.csv\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "RYsdNDAuHyav",
    "outputId": "082be187-fd49-4587-a6b8-e56e013b1db5"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Do paragraph-level enitity (character) extraction"
   ],
   "metadata": {
    "id": "kEjXZaRNVZYt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Install a spaCy model for text processing."
   ],
   "metadata": {
    "id": "ZcctyC2txmmg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!python3 -m spacy download en_core_web_md"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8P6rA-9Wp04",
    "outputId": "bf96ce6b-7b3e-4b35-f941-38c84c62465c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare a list of dictionaries, where key is the full character name (as identifier) and the value is a list of different ways this character may appear in text. This will be needed for setting rules for character detection"
   ],
   "metadata": {
    "id": "66Lsy_o9xb0Y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "characters_in_text_dict = dict()\n",
    "\n",
    "for _, row in pride_prej_chars_df.iterrows():\n",
    "  characters_in_text_dict[row['name']] = row['name_variants']\n",
    "\n",
    "# for k, v in characters_in_text_dict.items():\n",
    "#   print(f\"{k}: {v}\")"
   ],
   "metadata": {
    "id": "OXuwiF6XWvHP"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the NLP model and define a set of custom rules for character detection"
   ],
   "metadata": {
    "id": "bdquKuOTxxpu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spacy_pipeline = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# add the ruler BEFORE the ner\n",
    "ruler = spacy_pipeline.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "ruler.overwrite_ents = True\n",
    "\n",
    "# patterns to be used for identifying characters in the text\n",
    "patterns = []\n",
    "\n",
    "# # first, add general templates of the kind: Title + Capitalized Word\n",
    "# titles = [\"Mr.\", \"Mrs.\", \"Miss\", \"Lady\", \"Sir\", \"Rev.\", \"Colonel\"]\n",
    "# for title in titles:\n",
    "#     patterns.append({\n",
    "#         \"label\": \"PERSON\",\n",
    "#         \"pattern\": [{\"LOWER\": title.lower()}, {\"IS_TITLE\": True}]\n",
    "#     })\n",
    "\n",
    "# next, create patterns for specific name combinations\n",
    "for full_name, name_variants in characters_in_text_dict.items():\n",
    "  char_id = \"_\".join(full_name.split())\n",
    "  for name in name_variants:\n",
    "      patterns.append({\"label\": \"PERSON\", \"pattern\": name, \"id\":char_id})\n",
    "\n",
    "# load all the patterns into the ruler\n",
    "ruler.add_patterns(patterns)"
   ],
   "metadata": {
    "id": "Rkecg8OimePD"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# test the pipeline\n",
    "doc = spacy_pipeline(\"Mr. Bennet and George Wickham both knew Elizabeth Bennet.\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} ({ent.label_}) ({ent.id_})\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFwtNgXZqv1-",
    "outputId": "3911630e-2b33-426f-c269-3d71d3ece700"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Texts downloaded from Project Gutenbeg require some preprocessing to be ready for further analysis. Typical problems include almost randomly put newlines / tabs / multiple spaces as well as '--' connecting two words that should not be connected (e.g., \"Mrs. Bennet.--They\")"
   ],
   "metadata": {
    "id": "ikJMNL8dyAaE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def clean_gutenberg_paragraphs(raw_paragraphs):\n",
    "\n",
    "    cleaned_paragraphs = []\n",
    "\n",
    "    for par in raw_paragraphs:\n",
    "        # replace any internal newlines or tabs or multiple spaces with a single space\n",
    "        clean_par = re.sub(r'\\s+', ' ', par)\n",
    "\n",
    "        # replace the Gutenberg double-dash with a space-padded version\n",
    "        # this is to avoid problems with detection of entities (e.g., 'Mrs. Bennet.--They')\n",
    "        clean_par = clean_par.replace('--', ' -- ')\n",
    "\n",
    "        # strip leading/trailing whitespace\n",
    "        clean_par = clean_par.strip()\n",
    "\n",
    "        if clean_par: # if the paragraph isn't empty\n",
    "            cleaned_paragraphs.append(clean_par)\n",
    "\n",
    "    return cleaned_paragraphs"
   ],
   "metadata": {
    "id": "gBkSs2vpTBJ_"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function below extract characters from individual paragraphs of a book chapter by:\n",
    "* splitting the chapter into paragraphs\n",
    "* cleaning the text of those paragraphs\n",
    "* passing the paragraphs to the spaCy's NLP pipeline for further processing, specifically tokenisation and entity extraction based on custom rules (set above); not that the `parser` and `lemmatizer` are excluded from the pipeline since they are not needed for entity extraction; in addition, the `ner` module is also excluded since it proved not to work well with 19th century language (it eas trained on modern days web and news content)\n",
    "* filtering out the extracted entities to keep only those of the type PERSON."
   ],
   "metadata": {
    "id": "3LkE4maP-c7_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_characters_from_chapter(book_chapter: dict, nlp_pipeline) -> list:\n",
    "\n",
    "  chapter_entity_labels = []\n",
    "  chapter_entity_ids = []\n",
    "\n",
    "  chapter_pars = re.split(r'\\n\\n+', book_chapter['content'])\n",
    "\n",
    "  cleaned_chapter_pars = clean_gutenberg_paragraphs(chapter_pars)\n",
    "\n",
    "  processed_paragraphs = nlp_pipeline.pipe(cleaned_chapter_pars, disable=[\"ner\", \"parser\", \"lemmatizer\"])\n",
    "\n",
    "  for p in processed_paragraphs:\n",
    "      ent_labels = []\n",
    "      ent_ids = []\n",
    "      for ent in p.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "          ent_labels.append(ent.text.strip())\n",
    "          ent_ids.append(ent.id_)\n",
    "\n",
    "      chapter_entity_labels.append(ent_labels)\n",
    "      chapter_entity_ids.append(ent_ids)\n",
    "\n",
    "  chapter_pars_ents = []\n",
    "  for par_text, par_ents, par_ent_ids in zip(chapter_pars, chapter_entity_labels, chapter_entity_ids):\n",
    "    chapter_pars_ents.append({\n",
    "        'volume': book_chapter['volume'],\n",
    "        'chapter': book_chapter['chapter'],\n",
    "        'paragraph': par_text,\n",
    "        'entity_lbls': set(par_ents),\n",
    "        'entity_ids': set(par_ent_ids)})\n",
    "\n",
    "  return chapter_pars_ents"
   ],
   "metadata": {
    "id": "JOVJkqyToODV"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "chapters_with_entities = []\n",
    "\n",
    "for i, chapter in enumerate(chapters):\n",
    "  if i % 10 == 0: print(f\"processing {i+1}. chapter\")\n",
    "  chapters_with_entities.extend(extract_characters_from_chapter(chapter, spacy_pipeline))\n",
    "\n",
    "print(len(chapters_with_entities))"
   ],
   "metadata": {
    "id": "9EUQ_rVZW4jT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "062fecc7-e678-447e-d257-76e2ccf8d9f3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "chapters_with_entities_df = pd.DataFrame(chapters_with_entities)\n",
    "chapters_with_entities_df.head(10)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "6j4v_mHpqB7t",
    "outputId": "94d25531-7447-44d4-95e8-f83d19b9c8a4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code below is just for checking what PERSON type entities have been detected."
   ],
   "metadata": {
    "id": "lQwHfqdb_113"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# the following line shows how to turn a list of lists into a flat list\n",
    "# flattened_list = [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# all_extracted_entities = set([ent for paragraph_ents in chapters_with_entities_df.entity_lbls.to_list() for ent in paragraph_ents])\n",
    "\n",
    "# all_extracted_entity_ids = set([ent for paragraph_ents in chapters_with_entities_df.entity_ids.to_list() for ent in paragraph_ents])\n",
    "\n",
    "# print(\"\\n\".join(all_extracted_entities))\n",
    "# print(\"\\n\".join(all_extracted_entity_ids))\n",
    "# print(len(all_extracted_entity_ids))"
   ],
   "metadata": {
    "id": "EAqXEL9rrhYM"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Filter out the extracted entities\n",
    "\n",
    "We may need to filter out the extracted entities to keep only those who are really book characters. To that end, we will use the data about the book characters that were collected from the web.\n",
    "\n",
    "Note: this step is required only if we use some more general rules for entity detection - for example, rules of the type: Title + Capitalized Word, as given (and commented out) in the code above."
   ],
   "metadata": {
    "id": "5Fp3U-I61zOR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def check_entity(ent_label):\n",
    "  for full_name, name_variants in characters_in_text_dict.items():\n",
    "    if ent_label in name_variants:\n",
    "      return full_name\n",
    "  # if the entity label is not associated with any known entity\n",
    "  return None\n",
    "\n",
    "true_ents_in_pars = []\n",
    "\n",
    "for _, row in chapters_with_entities_df.iterrows():\n",
    "  if not row['entity_lbls'] or len(row['entity_lbls']) == 0:\n",
    "    continue\n",
    "\n",
    "  par_ents = []\n",
    "  for ent_lbl in row['entity_lbls']:\n",
    "    ent_id = check_entity(ent_lbl)\n",
    "    if ent_id:\n",
    "      par_ents.append(ent_id)\n",
    "\n",
    "  if len(par_ents) > 0:\n",
    "\n",
    "    vol_num = row['volume'].split()[1]\n",
    "    chapt_num = row['chapter'].split()[1]\n",
    "    vol_chapt = f\"{vol_num.strip('.')}_{chapt_num.strip('.')}\"\n",
    "\n",
    "    true_ents_in_pars.append({\n",
    "        'chapter': vol_chapt,\n",
    "        'paragraph': row['paragraph'],\n",
    "        'chars' : set(par_ents)\n",
    "    })"
   ],
   "metadata": {
    "id": "9ZmQF_W57cPS"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "true_ents_in_pars_df = pd.DataFrame(true_ents_in_pars)\n",
    "true_ents_in_pars_df.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ixAZxsFl-YBI",
    "outputId": "128ed460-dc90-4a7d-ae6a-bb0cbdfa209e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "true_ents_in_pars_df.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AhfqF5Bo-idL",
    "outputId": "45900dd1-c2ad-42c6-f2d1-d4a290bd5cf1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that in the cell below, the data frame is serialised using pickle since it could not be properly stored in a .csv file."
   ],
   "metadata": {
    "id": "C1CsauyEA74E"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# true_ents_in_pars_df.to_csv(\"chapters_with_paragraph_level_entities.csv\", index = False)\n",
    "\n",
    "with open(\"chapters_with_paragraph_level_entities.pkl\", \"wb\") as fobj:\n",
    "  pickle.dump(true_ents_in_pars_df, fobj)\n",
    "\n",
    "files.download(\"chapters_with_paragraph_level_entities.pkl\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "yY69p4HCq8jc",
    "outputId": "c0f7d628-fc60-495f-b832-2c7b0b0ac49f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a paragraph-level edge-list\n",
    "\n",
    "To create this edge list (and later network), we will establish connections between characters who appear in the same paragraph. If there is just one person mentioned in a paragraph, we simply skip the paragraph."
   ],
   "metadata": {
    "id": "bo9X74FSQC7R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# data_file = files.upload()\n",
    "\n",
    "# with open(\"chapters_with_paragraph_level_entities.pkl\", \"rb\") as fobj:\n",
    "#   true_ents_in_pars_df = pickle.load(fobj)\n"
   ],
   "metadata": {
    "id": "1pjselYR6zxH"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "edge_list = []\n",
    "\n",
    "for _, row in true_ents_in_pars_df.iterrows():\n",
    "  n_chars = len(row['chars'])\n",
    "  if n_chars < 2: continue\n",
    "\n",
    "  chars_list = list(row['chars'])\n",
    "\n",
    "  for i in range(n_chars-1):\n",
    "    for j in range(i+1, n_chars):\n",
    "      edge_list.append({\n",
    "          'chapter': row['chapter'],\n",
    "          'paragraph': row['paragraph'],\n",
    "          'source' : chars_list[i],\n",
    "          'target': chars_list[j]\n",
    "      })\n",
    "\n",
    "print(f\"Total number of edges: {len(edge_list)}\")\n",
    ""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U9UIItfDQH9p",
    "outputId": "23f0f3af-3916-44cf-e2d0-1ed76e774f12"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "edge_list_df = pd.DataFrame(edge_list)\n",
    "edge_list_df.head(10)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "mPrLfoAsSIhM",
    "outputId": "9981cf27-718d-49c4-d8ba-4911046075e6"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "weighted_edge_list = edge_list_df.groupby(['source','target']).paragraph.count()\n",
    "weighted_edge_list = weighted_edge_list.reset_index(drop=False)\n",
    "weighted_edge_list.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NpQaXvJRSa93",
    "outputId": "95858d99-97b6-4440-b20f-b4e46abfcd53"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "weighted_edge_list.rename(columns={'paragraph':'weight'}, inplace=True)\n",
    "weighted_edge_list.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "pBMvag7MSvCC",
    "outputId": "3e7881c5-3e78-4283-8df7-36fdd8f0b184"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "weighted_edge_list.sort_values(by='weight', ascending=False).head(10)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "7wffUp2Ibeke",
    "outputId": "e631451e-f5d2-4165-da86-edc79ed07a81"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create and visualise the (paragraph-level) network"
   ],
   "metadata": {
    "id": "xNIYnnojBbG0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "G_pars = nx.from_pandas_edgelist(df=weighted_edge_list,\n",
    "                                    source='source',\n",
    "                                    target='target',\n",
    "                                    edge_attr='weight',\n",
    "                                    create_using=nx.Graph)\n",
    "print(G_pars)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UIhxw90OS4Tq",
    "outputId": "2cb1bea4-b21f-445f-c0ab-6b3f3151b08c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# for edge in G_pars.edges(data=True):\n",
    "#   print(edge)"
   ],
   "metadata": {
    "id": "Y3Wp-CamU0RO"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add the character group (location-based one) as the node attribute. Add also a numrical representation of the group, as that will be useful later on for visualization"
   ],
   "metadata": {
    "id": "EAhD7q7UTAI2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "unique_char_groups = pride_prej_chars_df['group'].unique().tolist()\n",
    "groups_ids_mapping = {group:i for i, group in enumerate(unique_char_groups)}\n",
    "\n",
    "for node in G_pars.nodes():\n",
    "  loc_group = pride_prej_chars_df.loc[pride_prej_chars_df.name == node, 'group'].iloc[0]\n",
    "  loc_group_id = groups_ids_mapping[loc_group]\n",
    "\n",
    "  G_pars.nodes[node]['group_lbl'] = loc_group\n",
    "  G_pars.nodes[node]['group'] = loc_group_id"
   ],
   "metadata": {
    "id": "BVxz6lyvTI9X"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_graph(G,\n",
    "               graph_name,\n",
    "               graph_layout = None,\n",
    "               node_color_modifiers=None,\n",
    "               node_size_modifiers=None,\n",
    "               edge_weight_multiplier=1):\n",
    "    plt.figure(figsize=(12,11))\n",
    "\n",
    "    if graph_layout:\n",
    "      pos = graph_layout\n",
    "    else:\n",
    "      # pos = nx.kamada_kawai_layout(G)\n",
    "      pos = nx.spring_layout(G, seed=9, k=0.95, weight='weight')\n",
    "\n",
    "\n",
    "    if node_color_modifiers:\n",
    "        node_color = [node_color_modifiers[node] for node in G.nodes()]\n",
    "    else:\n",
    "        node_color = 'purple'\n",
    "\n",
    "    if node_size_modifiers:\n",
    "      node_size = [200 + 1500*node_size_modifiers[node] for node in G.nodes()]\n",
    "    else:\n",
    "      node_size = 350\n",
    "\n",
    "    edge_width = [attr['weight']*edge_weight_multiplier for (u, v, attr) in G.edges(data=True)]\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color=node_color, cmap='viridis')\n",
    "    nx.draw_networkx_edges(G, pos, width=edge_width, edge_color='silver')\n",
    "    nx.draw_networkx_labels(G, pos, font_color='indigo', font_size=9, font_weight='bold', horizontalalignment='left', verticalalignment='bottom')\n",
    "\n",
    "    plt.title(graph_name)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "Kz2IkZd5Extz"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plot_graph(G_pars,\n",
    "           \"Network of Pride and Prejudice actors, based on co-occurrence in book paragraphs\",\n",
    "          #  graph_layout=nx.kamada_kawai_layout(G_pars),\n",
    "           node_color_modifiers= nx.get_node_attributes(G_pars, 'group'),\n",
    "           edge_weight_multiplier=0.15)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "I8ufhIkoVS9R",
    "outputId": "79c59a57-3b85-4272-82da-c8200b4d076d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To examine the tendency of the network actors to group / cluster, we will compute key measures of actor connectedness and tendency to form groups in a network:"
   ],
   "metadata": {
    "id": "9tCOOIbLdsRg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from statistics import mean\n",
    "\n",
    "# network density\n",
    "den = nx.density(G_pars)\n",
    "# global clustering coefficient (transitivity)\n",
    "trans = nx.transitivity(G_pars)\n",
    "# local clustering coefficient\n",
    "loc_clust_coef = nx.clustering(G_pars)\n",
    "avg_loc_clust = mean(loc_clust_coef.values())\n",
    "\n",
    "print(f\"Density: {den:.4f}, transitivity (global clust. coeff.): {trans:.4f}, avg. local clust. coeff: {avg_loc_clust:.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fcqWsBudsDX",
    "outputId": "4625399b-12ec-4472-a3a9-ca7c413a71af"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute also node centralities. In this context, only degree and closeness centrality seem to be meaningful."
   ],
   "metadata": {
    "id": "AVd8gj9EB8OC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "degree_centr = nx.degree_centrality(G_pars)\n",
    "closeness_centr = nx.closeness_centrality(G_pars)"
   ],
   "metadata": {
    "id": "aql4kp8UeBWn"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plot_graph(G_pars,\n",
    "           \"Network of Pride and Prejudice actors, based on co-occurrence in book paragraphs\",\n",
    "          #  graph_layout=nx.kamada_kawai_layout(G_pars),\n",
    "           node_color_modifiers= nx.get_node_attributes(G_pars, 'group'),\n",
    "           node_size_modifiers=degree_centr,\n",
    "           edge_weight_multiplier=0.15)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "PVjitGL6eOhl",
    "outputId": "0eb57155-4d07-429e-dc45-2515ad088ea6"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a copy of the network for visualisation purposes, as pyvis changes some network attributes."
   ],
   "metadata": {
    "id": "MSfZ1Y-YCWSX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "G_pars_vis = G_pars.copy()\n",
    "\n",
    "nx.set_node_attributes(G_pars_vis, degree_centr, 'value')\n",
    "\n",
    "for u, v, attr in G_pars_vis.edges(data=True):\n",
    "    G_pars_vis[u][v]['width'] = attr['weight'] * 0.5"
   ],
   "metadata": {
    "id": "CSMRHqa_CgRq"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "net = Network(notebook = True, cdn_resources='remote', width=\"1000px\", height=\"700px\", bgcolor='#222222', font_color='white')\n",
    "\n",
    "net.from_nx(G_pars_vis)\n",
    "\n",
    "# net.show_buttons(filter_=['physics'])\n",
    "\n",
    "options = \"\"\"\n",
    "var options = {\n",
    "  \"physics\": {\n",
    "    \"barnesHut\": {\n",
    "      \"gravitationalConstant\": -15000,\n",
    "      \"centralGravity\": 0.3,\n",
    "      \"springLength\": 95\n",
    "    },\n",
    "    \"minVelocity\": 0.75,\n",
    "    \"stabilization\": {\n",
    "      \"enabled\": true,\n",
    "      \"iterations\": 1000\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "net.set_options(options)\n",
    "\n",
    "net.save_graph(\"pride_and_prejudice.html\")\n",
    "\n",
    "# Use IPython to display the file in the Colab cell\n",
    "IPython.display.HTML(filename=\"pride_and_prejudice.html\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "jZRs9rdri2fM",
    "outputId": "0e78fca8-bf68-479a-94da-49b78b7c650d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# for edge in G_pars_vis.edges(data=True):\n",
    "#   print(edge)"
   ],
   "metadata": {
    "id": "GydmZGN7CB8j"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Examine if clusters can be identified in the network"
   ],
   "metadata": {
    "id": "Cn1ClWZKe8fG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "louvaine_communities = nx.community.louvain_communities(G_pars, weight='weight')\n",
    "\n",
    "print(f\"Number of detected communities: {len(louvaine_communities)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rZVissQYfAAm",
    "outputId": "3886fa3c-c4f2-4164-9f41-7a00496e0f88"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "nx.community.modularity(G_pars, louvaine_communities)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPVwVu5ZhMWv",
    "outputId": "48831490-d7f7-48c6-ecae-82744016b0ef"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "louvain_partitions = {}\n",
    "for i, community in enumerate(louvaine_communities):\n",
    "    for node in community:\n",
    "        louvain_partitions[node] = i"
   ],
   "metadata": {
    "id": "nH67Ty-tfTR8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plot_graph(G_pars,\n",
    "           \"Communities detected in the Pride and Prejudice network, with Louvaine method\",\n",
    "          #  graph_layout=nx.kamada_kawai_layout(G_pars),\n",
    "           node_size_modifiers=degree_centr,\n",
    "           node_color_modifiers=louvain_partitions,\n",
    "           edge_weight_multiplier=0.15)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "DoUeb_t5fbG4",
    "outputId": "75dd3296-6a70-4788-a4df-7739f2a80b7b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "eb_communities = list(nx.community.girvan_newman(G_pars))\n",
    "partition_modularity_dict = {i : nx.community.modularity(G_pars, partition) for i, partition in enumerate(eb_communities)}\n",
    "best_partition_item = max(partition_modularity_dict.items(), key=lambda item: item[1])\n",
    "\n",
    "best_partition = eb_communities[best_partition_item[0]]\n",
    "eb_modularity = best_partition_item[1]\n",
    "\n",
    "print(f\"Modularity for Edge-betweenness method: {eb_modularity:.4f}\")\n",
    "print(f\"Number of clusters: {len(best_partition)}\")\n",
    "print(f\"Partitioning: {best_partition}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jETwD5SgSkM",
    "outputId": "751611f7-1346-43e4-a8a3-4b80567b5322"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a chapter-level network"
   ],
   "metadata": {
    "id": "asYLiTNXPr8A"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Identify set of entities for each chapter and eastablish a connection between any two of them"
   ],
   "metadata": {
    "id": "qJpS08dcAaDA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# data_file = files.upload()\n",
    "# true_ents_in_pars_df = pd.read_csv(\"chapters_with_paragraph_level_entities.csv\")"
   ],
   "metadata": {
    "id": "uOOic5sM7WZ-"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "chars_edge_list = []\n",
    "\n",
    "for chapter, chapter_group in true_ents_in_pars_df.groupby('chapter'):\n",
    "  all_chars = [char for par_chars in chapter_group['chars'] for char in par_chars]\n",
    "  all_unique_chars = list(set(all_chars))\n",
    "\n",
    "  n_chars = len(all_unique_chars)\n",
    "  if n_chars < 2:\n",
    "    continue\n",
    "\n",
    "  for i in range(n_chars-1):\n",
    "    for j in range(i+1, n_chars):\n",
    "      chars_edge_list.append({\n",
    "          'chapter': chapter,\n",
    "          'source' : all_unique_chars[i],\n",
    "          'target': all_unique_chars[j]\n",
    "      })\n",
    "\n",
    "print(len(chars_edge_list))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ak6PmNnT9_6R",
    "outputId": "f7df8279-75a1-47f2-f749-28e966c343f3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "chars_edge_list_df = pd.DataFrame(chars_edge_list)\n",
    "chars_edge_list_df.head(10)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "RGv2GRIkBU_q",
    "outputId": "5fa48f8d-f9c8-4d4b-9881-88718a12ffbf"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "weighted_edge_list = chars_edge_list_df.groupby(['source','target']).chapter.count()\n",
    "weighted_edge_list = weighted_edge_list.reset_index(drop=False)\n",
    "weighted_edge_list.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbqlLsyeBmlb",
    "outputId": "e8bfe8a4-bf6e-4bb1-d7e0-7bb48ebbbdee"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "weighted_edge_list.rename(columns={'chapter':'weight'}, inplace=True)\n",
    "weighted_edge_list.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "mkZ-2b_EDsbb",
    "outputId": "0bbee35d-e985-4cd7-e98f-fbb16283cddb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "weighted_edge_list.sort_values(by='weight', ascending=False).head(20)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "x0TFK2LbHudq",
    "outputId": "8755462e-c4da-461b-c3c5-9cf7bcd90f17"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create and explore the network"
   ],
   "metadata": {
    "id": "67FfcG5hEusJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "G_chapters = nx.from_pandas_edgelist(df=weighted_edge_list,\n",
    "                                    source='source',\n",
    "                                    target='target',\n",
    "                                    edge_attr='weight',\n",
    "                                    create_using=nx.Graph)\n",
    "print(G_chapters)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mGWLIODLEBw1",
    "outputId": "bde3c48e-0dd0-47f7-ae3b-9c4505d6cef2"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "warnings.simplefilter('ignore', UserWarning)\n",
    "\n",
    "plot_graph(G_chapters,\n",
    "           \"Network of Pride and Prejudice actors, based on co-occurrence in a chapter\",\n",
    "           edge_weight_multiplier=0.15)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "eKEPWOqeFGHi",
    "outputId": "a2100726-f95f-44e3-b119-21b3ad8173a7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from statistics import mean\n",
    "\n",
    "den = nx.density(G_chapters)\n",
    "trans = nx.transitivity(G_chapters)\n",
    "loc_clust_coef = nx.clustering(G_chapters)\n",
    "avg_loc_clust = mean(loc_clust_coef.values())\n",
    "\n",
    "print(f\"Density: {den:.4f}, transitivity (global clust. coeff.): {trans:.4f}, avg. local clust. coeff: {avg_loc_clust:.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XsCBB8Ud0HU",
    "outputId": "f2ef8622-8fa5-4090-838b-cd9e70295ba2"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Directions for future exploration\n",
    "\n",
    "Create a separate network for each book volume (6 in total), so that the evolution of the interaction of the book characters can be traced over time. Compare the networks in terms of the main descriptive statistics and observe how in distinct chapters distinct character groups (location-based groups) interact.\n"
   ],
   "metadata": {
    "id": "IDBIWRaeUtmb"
   }
  }
 ]
}
