{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## A Quick Introduction to Ollama",
   "id": "558cacc85defef45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ollama allows one to build LLM-based applications with open-source models like Llama, Gemma, or Mistral running on the local machine.\n",
    "\n",
    "There is also a seamless integration of Ollama with LangChain, so all we've learned so far about developing applications with LangChain is applicable when working with locally run models via Ollama.\n",
    "\n",
    "To integrate LangChain with Ollama, the following two steps are needed:\n",
    "1) Install Ollama from: [https://ollama.com/download](https://ollama.com/download)\n",
    "2) Download the models you want to use, as explained below.\n",
    "3) Use the downloaded model to create an instance of ChatOllama class\n",
    "\n",
    "Once you have Ollama installed, you can check that it's working correctly by opening command prompt (or terminal) and typing: <br>\n",
    "`ollama` <br>\n",
    "to get a list of available commands. For example, to see all models currently available on your machine, type (in command prompt / terminal):<br> `ollama list`.\n",
    "\n",
    "A list of models available via Ollama is given in the [models library](https://ollama.com/library). Once you've found the model you want to use, you can download it as follows:<br>\n",
    "`ollama pull <model_name>:<model_size>` <br>\n",
    "For example, to download the gemma3 model with 1B parameters: <br>\n",
    "`ollama pull gemma3:1b`\n",
    "\n",
    "If you no longer need a model, you can remove it as follows: <br>\n",
    "1) list the available models to identify the exact name of the one you want to remove: `ollama list` <br>\n",
    "2) remove the model by typing: `ollama rm <model_name>` <br>\n",
    "3) check that the model is removed: `ollama list`"
   ],
   "id": "e7b3a1b4e359cc96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### A few notes on choosing a model to run locally\n",
    "\n",
    "When choosing a model to run locally, pay attention to its size both in terms of the space required for its (local) storage and the RAM required for running it.\n",
    "\n",
    "Here are some general recommendations regarding the RAM required for efficiently running a local model:\n",
    "* 16GB RAM is recommended for 7B-8B models,\n",
    "* 32GB-64GB for 13B-30B models,\n",
    "* 128GB+ for 65B+ models.\n",
    "\n",
    "As for the space required for storing a model locally, consider that LLM size in GB is determined by multiplying the number of parameters (in billions) by the bytes per parameter, dictated by precision. This means that for storing 1B parameters with 16-bit precision, roughly 2GB is needed, and 0.5-1GB for 4/8-bit quantized models[*]. In addition, it is recommended to have 20-30% more memory than the calculated size (for context storage) to avoid \"out of memory\" errors.\n",
    "\n",
    "[*] Quantization refers to compressing the representation of model parameters from 16-bit to 8-bit or 4-bit; this significantly reduces memory usage with minimal quality loss.\n",
    "\n",
    "The following formulas can be used for estimating an LLM's memory size in GB:\n",
    "* 16-bit precision: Number of Parameters (in Billions) x 2\n",
    "* 8-bit: Number of Parameters (in Billions) x 1\n",
    "* 4-bit: Number of Parameters (in Billions) x 0.5"
   ],
   "id": "5624b8ec87be69ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once Ollama has started, it provides access to LLMs not only via command prompt (terminal) and the visual interface, but it also exposes an HTTP API on localhost (http://127.0.0.1:11434), so it can be accessed as any RESTFul service (e.g., using the requests library). This is also how LangChain access Ollama hosted models.",
   "id": "5f5bd30b907dd30c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "creative_llm = ChatOllama(\n",
    "    model=\"gemma3:4b\",\n",
    "    temperature=0.9\n",
    ")"
   ],
   "id": "a49093c23ff1fcd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "branding_system_msg = \"\"\"\n",
    "You are a highly creative assistant, with a plenty of original ideas. You especially excell in inventing catchy brand and product promotional messages.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", branding_system_msg),\n",
    "        (\"human\", \"What would be a good name for a company that makes and sells {product}? Suggest three distinct names and for each one provide a short explanation\"),\n",
    "    ]\n",
    ")"
   ],
   "id": "d7410b8ec7d59331",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "creative_question_chain = prompt_template | creative_llm\n",
    "\n",
    "response = creative_question_chain.invoke({\"product\": \"healthy energy drink\"})"
   ],
   "id": "f7ca77cbebb16164",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(response.content)",
   "id": "112b1225e3b608fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll follow LLM's suggestion to be more precise in our request:",
   "id": "ab37222c4a02f11a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "refined_response = creative_question_chain.invoke({\"product\": \"healthy energy drink based on superfoods, which provides vibrant energy\"})",
   "id": "c121b437021e422d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(refined_response.content)",
   "id": "e11260882f8b5818",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
