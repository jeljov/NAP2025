{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeljov/NAP2025/blob/main/LangChain_Zero_and_Few_Shot_Learning_for_Requirements_Tagging.ipynb)"
  },
  {
   "metadata": {
    "id": "m-vDxQcH17cp"
   },
   "cell_type": "markdown",
   "source": [
    "# Zero shot and Few shot learning for text classification (tagging)\n",
    "\n",
    "In this notebook we'll explore the use of LangChain for **few shot learning** in the context of text classification (tagging).\n",
    "\n",
    "**Few shot learning** is a form of machine learning where a model is taught to perform a particular task, such as classification or named entity recognition, by being provided with a few examples only, instead of a large training set, as is the case in 'traditional' machine learning tasks.\n",
    "\n",
    "As any other task based on large languge models (LLMs), few shot learning relies on the use of appropriate prompts. LangChain offers prompt template classes specifically built for few shot learning, which we will use to classify a set of software requirements into functional and non-functional.\n",
    "\n",
    "Note: there is also **zero-shot learning**, where the model is asked to performe a specific task without any examples being given, only guidance / instruction in natural language (prompt).\n",
    "\n",
    "\n",
    "We start by installing the required python packages..."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iRBO4Jwe17cr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4ba504f9-7986-42ce-f44f-5a4a5434ddd4"
   },
   "source": [
    "!pip -q install langchain langchain_community langchain_groq"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# adding packages for OpenAI's models in case we run out of free access to Groq\n",
    "# !pip install -q openai langchain-openai"
   ],
   "metadata": {
    "id": "nc96yvlNvtTw"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To select examples for few-shot learning, we will use [`faiss-cpu`](https://pypi.org/project/faiss-cpu/), a python implentation of the Facebook's [FAISS](https://github.com/facebookresearch/faiss) library for efficient similarity search and clustering of text embeddings"
   ],
   "metadata": {
    "id": "3K8NfKBvPA_M"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip -q install faiss-cpu"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "391i-4dUfapI",
    "outputId": "94d4d08c-241d-4c97-bd49-6d81768d460d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we need two libraries for the embeddings model we will use to create vector representation of examples for few-shot learning (such embeddings-based representation of examples is required for performing search of relevant examples, as will be explained later on)\n"
   ],
   "metadata": {
    "id": "JTwdGxDRQC3a"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip -q install sentence_transformers FlagEmbedding langchain-huggingface"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UWQAjwGKlxBa",
    "outputId": "0166bdb2-48be-46ff-e0a5-56732060ed18"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiate an LLM chat model\n",
    "\n",
    "As before, we will use Groq API to access Meta's LLama 3.1 8B model. More precisely, we will leverage [LangChain's integration with Groq API](https://python.langchain.com/docs/integrations/chat/groq/), which further simplifies instantiation of LLMs that Groq offers.\n",
    "\n",
    "To use Groq services, we need to load Groq API key into the current application environment (as shown below).\n",
    "\n",
    "For more details about LLama 3 model and access to it via Groq API, see [the LangChain Intro notebook](https://github.com/jeljov/NAP2025/blob/main/LangChain_Intro.ipynb)."
   ],
   "metadata": {
    "id": "6LHcu4-0fnCp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\",\n",
    "               temperature=0.0)"
   ],
   "metadata": {
    "id": "Mi0N5iXLD55B"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code below if instantiating an OpenAIChat model in case we run out of free tokens on Groq"
   ],
   "metadata": {
    "id": "sWnXj005Q61R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# llm = ChatOpenAI(model='gpt-4o-mini', temperature=0.0)"
   ],
   "metadata": {
    "id": "YPRhT8nVFaXc"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zero shot learning through prompting\n",
    "\n",
    "Let's see an example where we will try to instruct an LLM to classify software requirements into functional and non-fuctional.\n",
    "\n",
    "We start by checking if the model is 'aware' of these two main categories of software requirements:"
   ],
   "metadata": {
    "id": "g69-ckcFJPhA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are excellent software engineer with a lot of experience in gathering and managing software requirements.\n",
    "You assist the user by answering their questions about software requirements in a clear and concise manner. Restrict your answer to one paragraph only.\n",
    "\"\"\"\n",
    "\n",
    "user_message = \"What are {req_type} requirements?\"\n",
    "\n",
    "initial_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_message),\n",
    "    ('human', user_message),\n",
    "])\n",
    "\n",
    "simple_chain = initial_prompt | llm | StrOutputParser()"
   ],
   "metadata": {
    "id": "BDekhmbRZu_u"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "resp_functional_req = simple_chain.invoke('functional')\n",
    "print(resp_functional_req)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CWasQxY6aalX",
    "outputId": "bbe81924-114d-4551-9ff3-33a737957dbe"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will add a simple utility function that wraps the text so that it is easier to read the model's output:"
   ],
   "metadata": {
    "id": "XEcTFKzqNsBY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def wrap_and_print(text, width=100):\n",
    "  import textwrap\n",
    "  wrapped_text = textwrap.fill(text, width=width)\n",
    "  print(wrapped_text +'\\n')"
   ],
   "metadata": {
    "id": "iw5I1LNTNo2b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "wrap_and_print(resp_functional_req)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-bAszUoNxqd",
    "outputId": "e8badfcd-82b2-4757-f673-9b6b854c31f4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "resp_nonfunctional_req = simple_chain.invoke('non-functional')\n",
    "wrap_and_print(resp_nonfunctional_req)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nHP369PhdFCw",
    "outputId": "6408a9cf-8555-45ce-dfcc-ffe5cd31ada2"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the LLM is quite knowledgeable about software requirements, we may first try classification of requirements into functional and non-functional via **zero-shot learning**:"
   ],
   "metadata": {
    "id": "0HRPykMR3kvN"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F8nYsiCg17ct"
   },
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "zero_shot_system_msg = \"\"\"\n",
    "You are an experienced software engineer with expertise in requirements engineering. Please help the user classify software requirements into functional and non-functional.\n",
    "The user will give you a list of software requirements, each requirement starting in a new line. Classify each requirement in the user's list into one of the following two categories \"Functional\" or \"Non-functional\".\n",
    "Think through each requirement in the user's list before classifying it. If you don't know how to classify a requirement, assign to it the \"None\" label.\n",
    "Return your response as a JSON list of tuples, where the first element in the tuple is the text of a requirement from the user's list, while the second elements is the category assigned to that requirement (or \"None\" if you do no know how to classify it).\n",
    "Your response should include only this JSON list and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "zero_shot_user_msg=\"Software requirements to classify:\\n{requirements}\"\n",
    "\n",
    "zero_shot_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', zero_shot_system_msg),\n",
    "    ('human', zero_shot_user_msg),\n",
    "])\n",
    "\n",
    "zero_shot_simple_chain = zero_shot_template | llm | JsonOutputParser()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As an initial set of data for testing the zero-shot prompting, we'll use a small set of requirements in the form of user stories, for a (imaginary) music recommender mobile app:"
   ],
   "metadata": {
    "id": "PJ1zC82hd5AW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sample_requirements_list = [\n",
    "    \"I want to sign up with my email and password so I can access the app's features.\",\n",
    "    \"I want to create, name, and modify a custom playlist so I can organize my favorite tracks.\",\n",
    "    \"I want the music recommendation list to load in less than two seconds so I don't have to wait.\",\n",
    "    \"I expect all my listening history and personal data to be securely encrypted so my information is protected.\",\n",
    "    \"I want to be able to save a recommended song for offline playback so I can listen to it without an internet connection.\",\n",
    "    \"I need the app to be available 99.9% of the time so I can reliably listen to music during my travels.\",\n",
    "    \"I need the app's interface to look and behave consistently across all screens so it's intuitive and easy to use.\",\n",
    "    \"I want to be able to search for music by typing an artist name, song title, or genre so I can quickly find what I'm looking for.\",\n",
    "    \"I need the app to continue playing music if I briefly lose my cellular connection so my listening experience isn't interrupted.\",\n",
    "    \"I want the app to use no more than 100MB of storage for cached data so my phone doesn't run out of space.\",\n",
    "    \"I want to be able to 'Like' or 'Dislike' a recommended song so the system can improve future recommendations.\"\n",
    "]\n",
    "\n",
    "sample_requrements_str = \"- \" + \"\\n- \".join(sample_requirements_list)"
   ],
   "metadata": {
    "id": "Hbq9wjtZWBsJ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(zero_shot_template.format(requirements = sample_requrements_str))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V-80ZZekWdLU",
    "outputId": "0d0a538b-7165-46fb-c688-ad8bc0012f88"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "zero_shot_classes = zero_shot_simple_chain.invoke(sample_requrements_str)"
   ],
   "metadata": {
    "id": "Y-M2CsESiaml"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "type(zero_shot_classes)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AuYbrsCZX1iS",
    "outputId": "4cf2f9f4-d7fc-4c44-da08-dc39886367d0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for i, classified_req in enumerate(zero_shot_classes):\n",
    "  req, cls = classified_req\n",
    "  print(f\"{i+1}. {req}: {cls.upper()}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xDERqLaVMvJt",
    "outputId": "addc547d-bfc4-4bcd-d025-76dad3c0c7ad"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This worked well, but as few-shot generally performs better than zero-shot and this was just a tiny sample, we'll next explore few-shot learning."
   ],
   "metadata": {
    "id": "7qHZJywT-Fed"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Few shot learning"
   ],
   "metadata": {
    "id": "xwGSecnySAC-"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ie565Ehd17ct"
   },
   "source": [
    "The idea behind **few shot learning** is to provide an LLM with some additional knowledge and / or instruction how to use the knowledge it has, to be able to perform a particular task.\n",
    "\n",
    "Note: knowledge that an LLM 'develops' through training (both pre-training and instruction and preference tuning) is often referred to as **parametric knowledge**, whereas any additional knowledge and / or instruction it receives later, during the actual use is known as **contextual knowledge**.\n",
    "\n",
    "In case of Chat LLMs, this *contextual knowledge*, comes in the form of a few pairs of interactions between the user and the chatbot, serving as examples of what we expect the model to do after receiving an input from the user. It has been shown that in large majority of cases, with only a few examples (3-4) and sometimes even only one example, a model can be well instructed for almost any specific task.\n",
    "\n",
    "A nice quick intro to few shot learning (through prompting) is given in [this](https://youtu.be/JSBjj09xJeM?si=DJwLOeFHX7KOTGq2) short (5min) lecture from U. of Texas.\n",
    "\n",
    "One of the main challenges in few-shot learning is the selection of examples to give to the model to learn from, as these should be sufficiently represenative and distinct among themselves. LangChain offers support for example selection, as we'll explore next.  "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LangChain's support for few shot learning\n"
   ],
   "metadata": {
    "id": "-FeheUJD9Vxt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LangChain supports few shot learning for chat models throught a combination of two prompt template classes - [`ChatPromptTemplate`](https://reference.langchain.com/python/langchain_core/prompts/#langchain_core.prompts.chat.ChatPromptTemplate) and `FewShotChatMessagePromptTemplate`.\n",
    "\n",
    "`FewShotChatMessagePromptTemplate` is particularly useful as it allows us to seamlessly select examples to be included based on different criteria such as the length of the prompt or mutual similarity of the examples. It also allows for selecting examples based on the user's input (how similar examples are to the input). We'll now examine how such dynamic inclusion/exclusion of examples works.\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "8rjzEmL2OGhY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To learn about dynamic few-shot prompting (i.e., few-shot prompting with dynamic example selection) , we will use a dataset of software requirements with human-assigned functional / non-functional labels. The dataset orginates from the \"[Software requirements dataset](https://www.kaggle.com/datasets/iamvaibhav100/software-requirements-dataset)\" available from Kaggle.com\n",
    "\n",
    "We will start by loading the dataset (stored localy in the `software_requirements_processed.csv` file) and selecting a small sample of requirements to serve as examples for few shot learning, whereas the remaining ones will be used for evaluation purposes."
   ],
   "metadata": {
    "id": "n9NUjcZEzt-w"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "\n",
    "data_file = files.upload()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "Py7Hve3zYSSf",
    "outputId": "5e935535-399d-4c02-e824-65a2bef7308f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The uploaded object is a dictionary having the filename and content as itâ€™s key-value pair. So, we can extract the file name and read in the file content using pandas (to have it in the pandas DataFrame format)"
   ],
   "metadata": {
    "id": "Q7kxXW09YZXR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_name = list(data_file.keys())[0]\n",
    "req_data = pd.read_csv(file_name)\n",
    "req_data.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5iEYTKpAYg23",
    "outputId": "93d25471-da62-47f5-c2cf-327302430cd1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "req_data.head(10)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "gROAk_mb3y-f",
    "outputId": "6f8be7b1-7340-486f-99a2-ccd61087de6a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can examine the distribution of functional / non-functional requirements in this dataset:"
   ],
   "metadata": {
    "id": "7p66vCT_VHS5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "req_data.Category.value_counts(normalize=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "O91hAfYs09KZ",
    "outputId": "7a7a686d-873c-4671-810a-9203b74fba80"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we take a small random sample - say 50 requirements - from which we will select (a much smaller subset of) examples for few shot learning"
   ],
   "metadata": {
    "id": "Hk4pozUw1VcT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "rand_state = 9 # can be any number; used for initialising the random numbers generator\n",
    "req_ex_sample = req_data.sample(50, random_state=rand_state)\n",
    "req_ex_sample.Category.value_counts()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "eRB4zTcZ39_P",
    "outputId": "0d2866b3-3a61-4ecc-8418-4de3184156f1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMRghygF17cu"
   },
   "source": [
    "This random set of labelled requirements need to be turned into a list of examples, where each example is presented as a dictionary of two items: a requirement and its corresponding type (functional / non-functional), as exemplified below:\n",
    "\n",
    "```\n",
    "examples = [\n",
    "    {\n",
    "      \"requirement\": \"This is a placeholder for a real software requirement\",\n",
    "      \"type\": \"Functional\"\n",
    "    },\n",
    "    {\n",
    "      \"requirement\": \"This is a placeholder for a real software requirement\",\n",
    "      \"type\": \"Non-functional\"\n",
    "    },\n",
    "    #....\n",
    "]\n",
    "```\n",
    "To create such a list, we change the name of the data frame columns and transform the data frame into a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "req_ex_sample.rename(columns={'Requirement':'requirement', 'Category':'type'}, inplace=True)\n",
    "req_ex_list = req_ex_sample[['requirement','type']].to_dict('records')\n",
    "req_ex_list[:5]"
   ],
   "metadata": {
    "id": "kwQz47nr5b5U",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a34f1e7e-e6cd-4bab-d1cd-610725bf48ec"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To select a subset of examples to be used for few shot learning, we will use LangChain's `MaxMarginalRelevanceExampleSelector`, an example selector that selects examples so that they are highly similar to the input, while also optimizing for their mutual diversity. It does this by finding examples that have the highest similarity with the inputs, and then iteratively adding them while penalizing them for similarity to the already selected examples."
   ],
   "metadata": {
    "id": "DvfRO_KCVwxy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To compute similarity between the user's input and the examples as well as similarity among the examples, we need to represent the inputs and the examples in a numerical format, suitable for (similarity) computation. To that end, we will use **text embeddings** as currently the most widely used approach for representing textual content.  \n",
    "\n",
    "Specifically, we will use the [bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) open source model for creating text embeddings. It was selected due to its small size and good performance. It was developed by the Beijing Academy of AI and as other open source language models, it is available from the [HuggingFace Hub](https://huggingface.co/models).\n",
    "\n",
    "You may want to explore the embedding models leaderboard at HuggingFace: [https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard) to find alternative models you may want to try out.\n",
    "\n",
    "Note that LangChain directly supports large number of embedding models; for a list of supported models and instructions (code examples) how to use them, go to [Integrations > Embedding models](https://python.langchain.com/docs/integrations/text_embedding/) in LangChain documentation.\n",
    "\n",
    "A reminder: To access a model hosted at HuggingFace Hub, you'll need a HuggingFace (HF) access token. To obtain such a token, you should first create an account at [HuggingFace.co](https://huggingface.co/). Then, click on your profile in the top-right corner, then *Settings*, then *Access Tokens*, then *New Token*, set *Role* to *write* and *Generate*."
   ],
   "metadata": {
    "id": "25-obpRzjz50"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
    "\n",
    "model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "bge_embeddings = HuggingFaceEmbeddings(\n",
    "  model_name=model_name,\n",
    "  model_kwargs={'device': 'cpu'}, # tells it to use CPU; alternative is 'cuda' to run on GPU\n",
    "  encode_kwargs={'normalize_embeddings': True} # normalising embeddings as cosine similarity will be used later\n",
    ")"
   ],
   "metadata": {
    "id": "oirfG7URkrUv"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we instantiate the `MaxMarginalRelevanceExampleSelector`, with the BGE embeddings model and [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) as the vectors store.\n",
    "\n",
    "FAISS is an in-memory vector store, which allows for quick search of (embedded / vectorised) documents based on different similarity measures"
   ],
   "metadata": {
    "id": "IP-vWn1RobOl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.example_selectors.semantic_similarity import MaxMarginalRelevanceExampleSelector\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "mmr_example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "    # the list of examples available to select from\n",
    "    examples=req_ex_list,\n",
    "    # the embeddings model to be used to represent the examples, so that their semantic similarity can be computed\n",
    "    embeddings=bge_embeddings,\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over\n",
    "    vectorstore_cls=FAISS,\n",
    "    # The number of examples to select\n",
    "    k=5,\n",
    "    # the key of the example element used for search / selection;\n",
    "    # should not be used when this selector is used in FewShotChatMessagePromptTemplate (as we do further below)\n",
    "    # input_keys=['requirement']\n",
    ")"
   ],
   "metadata": {
    "id": "5cKzj3B-oaaw"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can first try it out"
   ],
   "metadata": {
    "id": "l_nXxr6xbInh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sample_input = {\"requirement\": \"A new user shall be able to navigate through the league and team pages within 30 seconds of reaching the start-up page.\"}\n",
    "\n",
    "selected_examples = mmr_example_selector.select_examples(sample_input)\n",
    "\n",
    "for ex in selected_examples:\n",
    "  print(f\"Requirement: \\\"{ex['requirement']}\\\": {ex['type']}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z8T8xe8cbGmq",
    "outputId": "e3ecfa8c-bff1-4cc5-de1d-92a1ff35e548"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can create a few shot prompt with examples selected by the MMR example selector. This task is facilitated by the LangChain's `FewShotChatMessagePromptTemplate` class, as shown below:"
   ],
   "metadata": {
    "id": "K7d-cm22VkYv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "\n",
    "few_shot_mmr_prompt = FewShotChatMessagePromptTemplate(\n",
    "    # setting the example selector we've created above\n",
    "    example_selector=mmr_example_selector,\n",
    "    # this is how we want to format the examples when we insert them into the prompt\n",
    "    example_prompt=ChatPromptTemplate.from_messages([\n",
    "        (\"user\", \"{requirement}\"),\n",
    "        (\"ai\", \"{type}\"),\n",
    "    ]),\n",
    "    # the variable(s) that will receive the user's input; it has to be named 'input'\n",
    "    input_variables=[\"input\"]\n",
    ")"
   ],
   "metadata": {
    "id": "O_uQpEdqpaxJ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "few_shot_system_msg = \"\"\"\n",
    "You are an experienced software engineer with expertise in requirements engineering. Please help the user classify software requirements into functional and non-functional.\n",
    "Think through the requirement the user gives you before responding.\n",
    "Limit your response to one term: \"Functional\" or \"Non-functional\".\n",
    "If you don't know the answer, respond with \"None\".\n",
    "\n",
    "Below are a couple of examples of classified requirements, to help you with the classification task.\n",
    "EXAMPLES:\"\"\"\n",
    "\n",
    "final_few_shot_mmr_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", few_shot_system_msg),\n",
    "        few_shot_mmr_prompt,\n",
    "        (\"user\", \"Requirement to classify:\\n{input}\"),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "id": "7sDmLEpiqumY"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "test_req = \"The current repair facility ratings shall be displayed to the user.\"\n",
    "\n",
    "print(final_few_shot_mmr_prompt.format(input=test_req))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pUYHh9aOtcIE",
    "outputId": "a4d33cd9-2f71-4c69-81f3-46edb85ed852"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "few_shot_mmr_chain = final_few_shot_mmr_prompt | llm | StrOutputParser()"
   ],
   "metadata": {
    "id": "6VYBEDLirLwa"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "few_shot_mmr_chain.invoke({\"input\":test_req})"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "yrsapx1YrOj9",
    "outputId": "4732319a-4c94-4cea-9e84-9502a46cbd75"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation with human labelled data"
   ],
   "metadata": {
    "id": "to25xRPiYvxv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now take the remaining part of the dataset we've loaded to examine how well the model performs the classification task. To that end, we will first drop instances that were used for examples selection (to avoid 'data leakage'):"
   ],
   "metadata": {
    "id": "F0Bumn1_RCDD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "example_indices = req_ex_sample.index.tolist()\n",
    "req_data.drop(example_indices, inplace=True)"
   ],
   "metadata": {
    "id": "9Tz2C_bp922u"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "req_data.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U_j1yt3Y-t7j",
    "outputId": "c9667aaa-72fc-4183-b8b7-5bdce93e1a06"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For evaluation purposes, we will take a random sample of requirements that the model has not seen. We are taking a small sample, in order not to overstep the available free time / tokens on Groq."
   ],
   "metadata": {
    "id": "iC7bx-GO_MYb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "eval_sample_size = 50\n",
    "\n",
    "req_eval_sample = req_data.sample(eval_sample_size, random_state=rand_state)\n",
    "req_eval_sample.reset_index(drop=True, inplace=True)\n",
    "req_eval_sample.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "On_cj2KwDLE6",
    "outputId": "d06c324a-9f8b-4b96-9b19-2c127037d81f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can pass the test requirements to the previously built `few_shot_mmr_chain` and add a new column to the data frame based on the obtained results:"
   ],
   "metadata": {
    "id": "ztyLhLRNiSVr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from time import sleep\n",
    "\n",
    "def llm_tagging(requirement):\n",
    "\n",
    "  try:\n",
    "    llm_response = few_shot_mmr_chain.invoke(requirement)\n",
    "    # print(llm_response)\n",
    "  except Exception as e:\n",
    "    print(f\"The following error occurred when tagging requirement \\\"{requirement}\\\": {e}\")\n",
    "    llm_response = None\n",
    "\n",
    "  # put the model to sleep for 5 sec\n",
    "  sleep(5)\n",
    "\n",
    "  return llm_response\n",
    "\n",
    "req_eval_sample['Predicted_category'] = req_eval_sample.Requirement.apply(llm_tagging)"
   ],
   "metadata": {
    "id": "uhTbOU65ihw6"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "req_eval_sample[['Requirement', 'Category', 'Predicted_category']]"
   ],
   "metadata": {
    "id": "aNw8pKpTjcqa",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "cbb1d624-25f6-40dc-b212-059b26744767"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check the level of matching between human labels and the labels assigned by the LLM:"
   ],
   "metadata": {
    "id": "VyNQO6QNjipi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from statistics import mean\n",
    "\n",
    "def label_match(row):\n",
    "  human_lbl = row['Category']\n",
    "  llm_lbl = row['Predicted_category']\n",
    "  return human_lbl == llm_lbl\n",
    "\n",
    "req_type_match = req_eval_sample.apply(label_match, axis=1)\n",
    "print(f\"Number of matched labels: {sum(req_type_match)}\")\n",
    "print(f\"Proportion of matched labels: {mean(req_type_match):.4f}\")"
   ],
   "metadata": {
    "id": "-HWltvJBjjlK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5572241f-a3c2-4a6b-e661-81527de80bfd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also compute the reqular classification eval metrics (confusion matrix, precision, recall, F1):"
   ],
   "metadata": {
    "id": "CeEYJ7PGtCqe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_confusion_matrix_stats(positive_cls:str, labelled_req: pd.DataFrame) -> dict:\n",
    "  TP = FP = TN = FN = 0\n",
    "  for _, row in labelled_req.iterrows():\n",
    "    true_val = row['Category']\n",
    "    pred_val = row['Predicted_category']\n",
    "    if true_val == pred_val:\n",
    "      if true_val == positive_cls: TP += 1\n",
    "      else: TN += 1\n",
    "    else:\n",
    "      if pred_val == positive_cls: FP += 1\n",
    "      else: FN += 1\n",
    "  return {\n",
    "      'TP': TP, 'FP': FP, 'TN':TN, 'FN': FN\n",
    "  }\n",
    "\n",
    "def compute_classification_metrics(tp:int, tn:int, fp:int, fn:int) -> dict:\n",
    "  precision = tp / (tp + fp)\n",
    "  recall = tp / (tp + fn)\n",
    "  f1 = 2 * precision * recall / (precision + recall)\n",
    "  return {\n",
    "      'precision': precision,\n",
    "      'recall': recall,\n",
    "      'F1': f1\n",
    "  }\n"
   ],
   "metadata": {
    "id": "f8r6Z_MuwaBH"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "positive_cls = 'Functional'\n",
    "\n",
    "cm_stats = compute_confusion_matrix_stats(positive_cls, req_eval_sample)\n",
    "print(cm_stats)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pFVOtTLDzNQO",
    "outputId": "ade614a6-4dde-46c9-f3a5-e0e6435ce70f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "cls_metrics = compute_classification_metrics(tp=cm_stats['TP'],\n",
    "                               tn=cm_stats['TN'],\n",
    "                               fp=cm_stats['FP'],\n",
    "                               fn=cm_stats['FN'])\n",
    "\n",
    "for metric, val in cls_metrics.items():\n",
    "  print(f\"{metric}: {val:.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wVYgjss8z9W3",
    "outputId": "2b6b9d75-b45f-4049-dbfd-666dbd6b3407"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's examine where the LLM made mistakes:"
   ],
   "metadata": {
    "id": "2uXchyPgsxLt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "req_eval_sample['correct_cls'] = req_type_match\n",
    "req_eval_sample.loc[~req_eval_sample.correct_cls, ['Requirement','Category','Predicted_category']]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "SbTTQRJvsx6J",
    "outputId": "b5c70965-a784-48c9-f81c-0154c52619de"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "fname = \"sample_requirements_labelled.csv\"\n",
    "req_eval_sample.to_csv(fname, index=False)\n",
    "files.download(fname)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "LFi_0uhIm9Ai",
    "outputId": "78e9326c-b197-4fd6-dcc5-0feba86f3874"
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
   }
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
