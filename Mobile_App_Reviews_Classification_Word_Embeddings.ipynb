{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeljov/NAP2025/blob/main/Mobile_App_Reviews_Classification_Word_Embeddings.ipynb)"
  },
  {
   "metadata": {
    "id": "SyHS7pwnH9Ko"
   },
   "cell_type": "markdown",
   "source": "# Word embeddings for document (review) classification"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtRC_BvTclbi"
   },
   "source": [
    "This notebook exemplifies text classification using a pre-trained word embeddings model ([Word2Vec](https://en.wikipedia.org/wiki/Word2vec)) for transforming textual data into numerical vectors (to serve as the input for classification algorithms).<br>\n",
    "\n",
    "The dataset used in the example is the same as the one used in the previous two classes. It originates from the paper [Listening to the Crowd for the Release Planning of Mobile Apps](https://ieeexplore.ieee.org/abstract/document/8057860) and is available from [this web page](https://dibt.unimol.it/report/others/clap/) with the supplementary materials for the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The initial setup: downloading the required models, loading packages, seting up some useful variables\n",
    "\n",
    "Up to now we’ve been using spaCy’s smallest English language pipeline (_en_core_web_sm_), which provides vocabulary, syntax, and entities, but not vectors. To make advantage of the spaCy's built-in word vector model we’ll need a larger pipeline, namely either _en_core_web_md_ or _en_core_web_lg_. An overview of all the NLP piplines is available at [https://spacy.io/models/en](https://spacy.io/models/en)\n",
    "\n",
    "Since neither medium nor large spaCy's english language pipelines are directly available from Colab, we will first download the one we will use (_en_core_web_lg_):"
   ],
   "metadata": {
    "id": "_7Y85LBzIWbf"
   }
  },
  {
   "cell_type": "code",
   "source": "!python -m spacy download en_core_web_lg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_QTdtLKYTIIN",
    "outputId": "38553e2b-3383-4db4-8692-315faac3cbe7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note: in case of running this notebook locally, put the following line in comments and run the following line in Terminal or Command Prompt: <br>\n",
    "`python -m spacy download en_core_web_lg`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BCajUlNtH9LC"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# modules for oversampling and pipeline building\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# sklearn modules that implement classification algorithms\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import spacy\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "RAND_STATE = 1\n",
    "\n",
    "# spacy pipeline to use for text preprocessing and word vectors\n",
    "SPACY_MODEL = \"en_core_web_lg\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owy9VrttH9LX"
   },
   "source": [
    "### Explore spaCy's pre-trained word embeddings model (Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQ0yKI096ecA"
   },
   "source": [
    "Load the spaCy pipeline with pre-trained Word2Vec word embeddings (vectors):"
   ]
  },
  {
   "metadata": {
    "id": "h74VDvO5RGPk"
   },
   "cell_type": "code",
   "source": [
    "spacy_model = spacy.load(SPACY_MODEL)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eflZBv1CH9L6"
   },
   "source": [
    "The loaded NLP pipeline includes a vocabulary of 343K unique words represented with 300-dimensional vectors.\n",
    "\n",
    "Let's see what a word embedding looks like:"
   ]
  },
  {
   "metadata": {
    "id": "Iq8GdSAIRGPm"
   },
   "cell_type": "code",
   "source": [
    "world_vec = spacy_model.vocab['world'].vector"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "eWGatJ3oRGPn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "abc87c2a-3495-4043-dc7e-43b21c8813da"
   },
   "cell_type": "code",
   "source": [
    "print(f\"Vectors dimension: {len(world_vec)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QKrGp9lRH9L6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2389debe-7ebd-41a8-a0a5-265d39f1072c"
   },
   "source": [
    "print('World: ', world_vec)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before moving further, let's just check how we access words in the vocabulary of a spaCy's NLP pipeline:"
   ],
   "metadata": {
    "id": "zX3jswSmQa_a"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "type(spacy_model.vocab['world'])"
   ],
   "metadata": {
    "id": "8MLNgpsoQpse",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bdc12f82-fcee-4c38-f428-126851b2ee49"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each unique word in spaCy's vocabulary is represented as an instance of the [`Lexeme`](https://spacy.io/api/lexeme) class.\n",
    "\n",
    " A Lexeme holds context-free lexical attributes - like if the word is a number, if it's a stop-word, its string, its word vector, etc:"
   ],
   "metadata": {
    "id": "4FzVRHUjQqMR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "lexeme = spacy_model.vocab['happy']\n",
    "print(\"Properties of the word \\\"happy\\\" as a Lexeme\")\n",
    "print(lexeme.text)\n",
    "print(lexeme.like_num)\n",
    "print(lexeme.is_stop)\n",
    "print(lexeme.has_vector)"
   ],
   "metadata": {
    "id": "S81RI41fSFUd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7669d819-98ec-4c12-95e5-cc031005734b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "On the other hand, `Lexeme` lacks contextual linguistic information - such as part-of-speech and lemma, which are available from a [`Token`](https://spacy.io/api/token):"
   ],
   "metadata": {
    "id": "zg0dOv2l-1cH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "doc = spacy_model(\"I'm so happy!\")\n",
    "print(\"Properties of the word \\\"happy\\\" as a Token\")\n",
    "token = doc[3]\n",
    "print(token.lemma_)\n",
    "print(token.pos_)\n",
    "print(token.text)\n",
    "print(token.lex.is_stop) # access to the lexeme from the token"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jHkR4T9B-4-E",
    "outputId": "2e26255c-8bab-43bf-dcd4-18ffa2a43a2b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "YUCRYuUDRGPp"
   },
   "cell_type": "markdown",
   "source": [
    "### Word similarity"
   ]
  },
  {
   "metadata": {
    "id": "_ZEKKWN8RGPq"
   },
   "cell_type": "markdown",
   "source": [
    "Having words represented as vectors, we can use linear algebra and vector space models to analyze the relationship between words.\n",
    "\n",
    "We will start by examining word similarity, by computing the ***cosine similarity*** of word vectors. Cosine similarity is a widely used measure of semantic similarity of words. Its value ranges from -1 to 1, though it is usually used in the non-negative space [0, 1] where 0 means absence of similarity and 1 means extremely similar or identical.\n",
    "\n",
    "In spaCy, we can use the built-in similarity function (which implements cosine similarity) to calculate word similarity based on their vectors:"
   ]
  },
  {
   "metadata": {
    "id": "CMmdaIuURGPq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ccd3b3d3-08c5-454e-e719-76ae5d9ee208"
   },
   "cell_type": "code",
   "source": [
    "snow = spacy_model.vocab['snow']\n",
    "winter = spacy_model.vocab['winter']\n",
    "summer = spacy_model.vocab['summer']\n",
    "\n",
    "\n",
    "print(f\"Similarity of snow and winter: {snow.similarity(winter)}\")\n",
    "print(f\"Similarity of snow and summer: {snow.similarity(summer)}\")\n",
    "print(f\"Similarity of winter and summer: {summer.similarity(winter)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "spain = spacy_model.vocab['Spain']\n",
    "portugal = spacy_model.vocab['Portugal']\n",
    "\n",
    "print(f\"Similarity of Spain and Portugal: {spain.similarity(portugal)}\")\n",
    "print(f\"Similarity of Spain and snow: {spain.similarity(winter)}\")"
   ],
   "metadata": {
    "id": "bpBHZxfWTogq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "950b16c0-7ff9-4941-e07e-2160701a8c42"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "KFX3cB_ZRGPr"
   },
   "cell_type": "markdown",
   "source": [
    "### Word analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9bdYNYuH9L7"
   },
   "source": [
    "Word embeddings are well known for their good performance on the analogies task, that is, modeling analogy relationships between words.\n",
    "The most famous example is:<br>\n",
    "_Man is to Woman as King is to Queen_ <br>\n",
    "\n",
    "This can also be writen as:\n",
    "Man - Woman = King - Queen\n",
    "\n",
    "With some simple arithmetics, the above can also be written as follows: <br>\n",
    "King + Woman – Man = Queen <br>\n",
    "\n",
    "and can be interpreted as follows: adding the vectors associated with the words *king* and *woman* while subtracting *man* results in a vector that is the most similar to the vector for *queen*. In other words, by subtracting the concept of man from the concept of King we get a representation of the \"royalty\". Then, if we add the concept of woman, the concept we obtain is closest to the word \"queen\"."
   ]
  },
  {
   "metadata": {
    "id": "ZtwEZCpORGPr"
   },
   "cell_type": "code",
   "source": [
    "def get_vector(word:str):\n",
    "  return spacy_model.vocab[word].vector\n",
    "\n",
    "king_vec = get_vector('king')\n",
    "queen_vec = get_vector('queen')\n",
    "man_vec = get_vector('man')\n",
    "woman_vec = get_vector('woman')\n",
    "\n",
    "res_vec = king_vec - man_vec + woman_vec"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "6DvH7KOKRGPs"
   },
   "cell_type": "markdown",
   "source": [
    "Function for computing cosine similarity of two vectors\n",
    "($cos(A,B) = (A · B) / (||A|| * ||B||)$):"
   ]
  },
  {
   "metadata": {
    "id": "sBrWnL4URGPs"
   },
   "cell_type": "code",
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "cosine = lambda v1, v2: np.dot(v1, v2) / (norm(v1) * norm(v2))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And another one for determining topK most similar words to the given word (i.e., its word vector):"
   ],
   "metadata": {
    "id": "3kXaWUggVv0-"
   }
  },
  {
   "metadata": {
    "id": "R_RAe5l1RGPu"
   },
   "cell_type": "code",
   "source": [
    "def most_similar_words(word_vec, words_to_exclude=None, topk=5):\n",
    "    spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "    if words_to_exclude:\n",
    "        words_to_exclude.extend(spacy_stopwords)\n",
    "    else:\n",
    "        words_to_exclude = spacy_stopwords\n",
    "\n",
    "    print(len(words_to_exclude))\n",
    "\n",
    "    # from the vocabulary select all words eligible for a comparison\n",
    "    all_words = [w for w in spacy_model.vocab if\n",
    "                w.has_vector and (w.text not in words_to_exclude) and w.is_lower and (len(w.text) > 3)]\n",
    "\n",
    "    # sort the selected (eligible) words by similarity\n",
    "    candidates = sorted(all_words, key=lambda w: cosine(word_vec, w.vector), reverse=True)\n",
    "\n",
    "    return candidates[:topk]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "HD6g_EV9RGPt",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ab79b5a3-d135-4c44-ef37-3296847863c1"
   },
   "cell_type": "code",
   "source": [
    "print('Similarity between queen and result:', cosine(res_vec, queen_vec))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "y2hCYHBeRGPu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "db1060c1-d6ad-4faf-bb3c-afa6216a6692"
   },
   "cell_type": "code",
   "source": [
    "print(\"Words most similar to the resulting vector:\", [c.text for c in most_similar_words(res_vec, words_to_exclude=['king','man','woman'])])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "s9xBpvCZRGPu"
   },
   "cell_type": "markdown",
   "source": [
    "We can try the same for <br>\n",
    "*Paris is to France as Madrid is to ?* <br>\n",
    "or <br>\n",
    "Paris - France = Madrid - ?"
   ]
  },
  {
   "metadata": {
    "id": "jv4p9kHxRGPv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a959dc31-b6e8-4a82-d9a4-708fe095377e"
   },
   "cell_type": "code",
   "source": [
    "paris_vec = get_vector('paris')\n",
    "france_vec = get_vector('france')\n",
    "madrid_vec = get_vector('madrid')\n",
    "spain_vec = get_vector('spain')\n",
    "\n",
    "res_vec_2 = madrid_vec - paris_vec + france_vec\n",
    "\n",
    "print(f\"Similarity of spain and the resulting vector: {cosine(res_vec_2, spain_vec)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "2d2_x0fORGPv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0b859821-2f1d-434a-fd52-d16c9af6b2ba"
   },
   "cell_type": "code",
   "source": [
    "print(\"Words most similar to the resulting vector:\", [c.text for c in most_similar_words(res_vec_2, words_to_exclude=['france','madrid','paris'])])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "oR6fw5YrRGPw"
   },
   "cell_type": "markdown",
   "source": [
    "You may want to try similar for:\n",
    "* France - Paris + Rome =\n",
    "* France - french + english =\n",
    "* December - November + June =\n",
    "* Man - Woman + Sister ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBf7_zE4H9L-"
   },
   "source": [
    "#### Visualization of Word Embeddings\n",
    "\n",
    "Visualisation is often used to get a better understanding of how word vectors model relations among words.\n",
    "\n",
    "Since word vectors are high dimensional - Word2Vec has 300 dimensions and more recent models have a few thousand dimensions - we cannot visualize them directly.\n",
    "A typical approach to overcome that challenge is to apply dimensionality reduction algorithm and visualize thus transformed data.\n",
    "Next time, we will use [t-SNE](https://lvdmaaten.github.io/tsne/), a popular dimension reduction technique to reduce the word vectors to 2D and explore the word relations to see if we can find some pattern visually.\n",
    "<br>\n",
    "For now, we will explore an interactive visualization of word embeddings can be found here:\n",
    "[https://projector.tensorflow.org/](https://projector.tensorflow.org/)"
   ]
  },
  {
   "metadata": {
    "id": "wR8iOe9eH9MC"
   },
   "cell_type": "markdown",
   "source": [
    "## Use word embeddings for text classification"
   ]
  },
  {
   "metadata": {
    "id": "WeAO7VkiH9MK"
   },
   "cell_type": "markdown",
   "source": [
    "### Load and prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIAUmAvjIdXS"
   },
   "source": [
    "We use the mobile apps reviews dataset that was downloaded from the above-linked web page and stored locally."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "\n",
    "data_files = files.upload()\n",
    "data_file_path = list(data_files.keys())[0]"
   ],
   "metadata": {
    "id": "eXzDHEDfW028",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "outputId": "ceb74107-6ab2-4b53-ba0d-98878b63bbb7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If running the code locally, comment the code in the cell above and uncomment the code in the following cell"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from pathlib import Path\n",
    "#\n",
    "# data_file_path = Path.cwd() / 'data' / 'mobile_apps_reviews.csv'"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(data_file_path)\n",
    "data.head()"
   ],
   "metadata": {
    "id": "InYHTM7zxGWe",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "outputId": "ae8010a1-4253-4e68-9cb7-44d1a70998a5"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pycNZt1UH9MZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fe526529-65f6-4fd9-f0bc-134c6a0742d5"
   },
   "source": [
    "data.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "data.category.unique()"
   ],
   "metadata": {
    "id": "uVsspAxlH4JY",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6a057b08-3756-4299-b82f-d27da5b43f37"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will re-code the outcome variable as in the previous classes, in particular:\n",
    "\n",
    "* functional: BUG, FEATURE\n",
    "* non-functional: PERFORMANCE, USABILITY, ENERGY, SECURITY\n",
    "* irrelevant: OTHER\n"
   ],
   "metadata": {
    "id": "NX9kNQ64HcHr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def category_to_cls(cat):\n",
    "  if cat == \"OTHER\": return cat\n",
    "  if cat in ['BUG', 'FEATURE']: return \"FUNCTIONAL\"\n",
    "  return \"NON-FUNCTIONAL\"\n",
    "\n",
    "data['cls'] = data.category.apply(category_to_cls)\n",
    "data.cls.value_counts(normalize=True)"
   ],
   "metadata": {
    "id": "a3nFO75EHw23",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "outputId": "746307a2-fecd-46e7-d874-b04e6ebbff19"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add review length, as in the previous classes"
   ],
   "metadata": {
    "id": "7g3tCEN3KAP1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data['review_len'] = data.review.apply(len)"
   ],
   "metadata": {
    "id": "skeQP6j8KFV3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "data.head()"
   ],
   "metadata": {
    "id": "4kACKWkfJJr7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "outputId": "38a87a8d-9bcc-45b5-93fb-1766475a46fe"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLwD3zBBH9Mf"
   },
   "source": [
    "Keep the review, review length, and rating columns (as in the previous classes) and split the data set into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LcOeik8hH9Mf"
   },
   "source": [
    "X = data[['review','review_len','rating']]\n",
    "y = data['cls']\n",
    "\n",
    "# do label encoding, not to work with strings\n",
    "y_encodings, y_levels = pd.factorize(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encodings, test_size = 0.2, random_state = RAND_STATE, stratify=y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOOYdpyCH9Mf"
   },
   "source": [
    "### Use word embeddings for document representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILIK_gfDH9Mg"
   },
   "source": [
    "We have a way to numerically represent each word - the embeddings provide a vector for every word in our vocabulary.<br>\n",
    "However, to do the classification, we need to have a numerical representation of complete reviews (documents). So, for each review, we need a vector that would represent that review and that vector should be based on embeddings of the words in the review. There are several options to do that; here, we apply a very simple method: we average the embeddings of the words forming the review.<br>\n",
    "Though simple, this method often works well for short texts such as the reviews we are working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhcmSIu0H9Mh"
   },
   "source": [
    "#### Creating a custom transformer class\n",
    "\n",
    "We will create a class - `ReviewVectorizer` - that transforms every review into a vector representation, as described above, using a pre-trained Word2Vec model (that is integrated in the spaCy NLP pipeline). This transformation is preceded by text 'cleaning' and reducing it to semantics-bearing words, so that only 'the most important' words are used for building a text (review) representation.\n",
    "\n",
    "Note that the `ReviewVectorizer` class is defined as a subclass of sklearn's `BaseEstimator` and `TransformerMixin`. This is required in order to make our\n",
    "transformer component compatible with the sklearn's tools such as GridSearchCV and Pipeline.\n",
    "\n",
    "When creating a custom transformer by inheriting from `BaseEstimator` and `TransformerMixin`, we are primarily required to implement the following methods:\n",
    "* `__init__(self, *args, **kwargs)`. The `__init__`'s role is to initialize the transfomer. **Important**: all hyperparameters of our custom transformer that can be set by the user must be defined as the input arguments of the `__init__` method and stored as instance attributes.\n",
    "* `fit(self, X, y=None)`. The purpose of this method is to learn any necessary parameters from the training data, $X$; for a transformer, this usually involves calculating statistics like the mean or standard deviation. It must return self.\n",
    "* `transform(self, X)`. The role of this method is to apply the transformation to the input data, $X$, using the parameters learned in `fit`. It must return the transformed data.\n",
    "\n",
    "\n",
    "If interested to learn more, the sklearn's documentation (in particular, [this page](https://scikit-learn.org/stable/developers/develop.html#rolling-your-own-estimator)) provides detailed information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qHvVwVi5H9Mh"
   },
   "source": [
    "class ReviewVectorizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, spacy_pipeline):\n",
    "\n",
    "        # spacy language model (NLP pipeline) to be used\n",
    "        self.spacy_pipeline = spacy_pipeline\n",
    "\n",
    "        # dimensionality of vectors\n",
    "        self.D = len(self.spacy_pipeline.vocab['word'].vector) if self.spacy_pipeline else None\n",
    "\n",
    "\n",
    "    def spacy_tokenizer(self, doc):\n",
    "        # Create token objects with linguistic annotations\n",
    "        tokens = self.spacy_pipeline(doc)\n",
    "\n",
    "        # remove punctuation, spaces, numbers, and stopwords\n",
    "        tokens = [token for token in tokens if ((not token.is_punct) and (not token.like_num) and (not token.is_space) and (not token.is_stop))]\n",
    "\n",
    "        # Lemmatize each token and convert each token into lowercase\n",
    "        words = [token.lemma_.lower() for token in tokens]\n",
    "\n",
    "        return words\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        # dictionary of review words that could not be matched against the spacy model\n",
    "        # key is the review ordinal number, value is a list of unmatched words\n",
    "        unmatched_words = defaultdict(list)\n",
    "\n",
    "        # indices of documents that could not be represented as vectors due to\n",
    "        # their words not having vectors in the used word vector model\n",
    "        dropped_indices = []\n",
    "\n",
    "        # create a zero-matrix with the number of rows corresponding to the number of documents and\n",
    "        # number of columns corresponding to the dimensions of the spacy's word vectors\n",
    "        X_transformed = np.zeros((len(X), self.D))\n",
    "\n",
    "        for i, text in enumerate(X):\n",
    "            words = self.spacy_tokenizer(text)\n",
    "            vecs = []\n",
    "            for word in words:\n",
    "                try:\n",
    "                    word_vec = self.spacy_pipeline.vocab[word].vector # throws KeyError if word is not found\n",
    "                    vecs.append(word_vec)\n",
    "                except KeyError:\n",
    "                    unmatched_words[i].append(word)\n",
    "                    print(f\"No vector for word: {word}\")\n",
    "            if len(vecs) > 0:\n",
    "                vecs = np.array(vecs)\n",
    "                X_transformed[i] = vecs.mean(axis=0)\n",
    "            else:\n",
    "                dropped_indices.append(i)\n",
    "\n",
    "        self.report_unmatched_words_stats(dropped_indices, unmatched_words)\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def report_unmatched_words_stats(dropped_docs_list, unmatched_words_dict):\n",
    "      from statistics import mean, stdev\n",
    "\n",
    "      unmatched_counts = [len(unmatched) for unmatched in unmatched_words_dict.values()]\n",
    "      if len(unmatched_counts) > 0:\n",
    "        print(f\"Average number of unmatched words per review (and SD): {mean(unmatched_counts):.4f} ({stdev(unmatched_counts):.4f})\")\n",
    "\n",
    "      dropped_cnt = len(dropped_docs_list)\n",
    "      if dropped_cnt > 0:\n",
    "        print(f\"Number and proportion of reviews with no words found: {dropped_cnt}({dropped_cnt} / {len(X)})\")\n",
    "\n",
    "\n",
    "    # def fit_transform(self, X, y=None, **fit_params):\n",
    "    #     return self.transform(X)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1Jce_o1H9Mk"
   },
   "source": [
    "### Train classifiers on the document embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-2rCdlJH9Mh"
   },
   "source": [
    "Use the `ReviewVectorizer` class to transform reviews into vectors using a word embeddings model.\n",
    "\n",
    "Use [`RobustScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) to rescale the two additional features (using median and inter-quartile range) so that all the features are in the roughly equal value range.\n",
    "\n",
    "Then we can use the resulting feature set to build a classifier."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load models required for the ReviewVectorizer; if not already loaded\n",
    "# spacy_model = spacy.load(SPACY_MODEL)\n",
    "\n",
    "# Instantiate the review vectoriser\n",
    "vectorizer = ReviewVectorizer(spacy_pipeline=spacy_model)\n",
    "\n",
    "# Instatiate a transformer to rescale (normalise) the other two features (rating and review length)\n",
    "normaliser = RobustScaler()\n",
    "\n",
    "# Apply transformers to all columns\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('embeddings', vectorizer, 'review'),\n",
    "        ('other_features', normaliser, ['review_len','rating']),\n",
    "        ])\n"
   ],
   "metadata": {
    "id": "MNuiq03UWFcq"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2peG2HkH9Mm"
   },
   "source": [
    "We will use **Random Forest** classification method to build a classifier. The rationale: in a [comparative analysis](https://dl.acm.org/doi/10.5555/2627435.2697065) performed with 179 general purpose classification algorithms on 121 different datasets, Random forest was one of the top performing algorithms.\n",
    "\n",
    "For a quick introduction to Random Forest, watch [this video tutorial](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ).\n",
    "\n",
    "We are also using [SMOTE resampling method](https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/) to address the problem of imbalanced dataset (recall that the three review categories (classes) are not equally present). **SMOTE** is abbreviation from *Synthetic Minority Oversampling Technique*, and is a technique for increasing the size of a minority class (and thus balancing the class distribution) by synthesizing new instances of that class from the existing instances. In short, SMOTE works as follows:\n",
    "\n",
    "A random instance from the minority class is first chosen. Then k of the nearest neighbors for that instance are found (typically k=5). A randomly selected neighbor is chosen and a synthetic instance is created at a randomly selected point between the two instances in feature space.\n",
    "\n",
    "Finally, note that we are using Pipeline from the `imblearn.pipeline` (not from `sklearn.pipeline` as in the previous classes), since this kind of pipepine properly handles class balancing methods (apply them to the train set, but not to the test set)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "rf_classifier = RandomForestClassifier(random_state=RAND_STATE, n_estimators=100)\n",
    "\n",
    "smote = SMOTE(random_state=RAND_STATE)\n",
    "\n",
    "rf_pipe = Pipeline(\n",
    "    [\n",
    "        ('vectorizer', column_transformer),\n",
    "        ('resampling', smote),\n",
    "        ('classifier', rf_classifier)\n",
    "     ],\n",
    "     verbose=True)\n",
    "\n",
    "rf_pipe.fit(X_train, y_train)"
   ],
   "metadata": {
    "id": "U6ktNbBGb1BH",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "outputId": "08b4275e-9605-422c-e606-ef30a7140f82"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kQH4W9xH9Mm"
   },
   "source": [
    "#### Evaluate the Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "ConfusionMatrixDisplay.from_estimator(\n",
    "        rf_pipe,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        display_labels=y_levels,\n",
    "        cmap='Greens',\n",
    "    );"
   ],
   "metadata": {
    "id": "-cTxtd28SfnN",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "outputId": "478d3b57-b1ed-4607-8098-38b3b8cffb4b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "rf_predictions = rf_pipe.predict(X_test)\n",
    "\n",
    "perf_rf = metrics.classification_report(y_true = y_test, y_pred = rf_predictions)\n",
    "print(perf_rf)"
   ],
   "metadata": {
    "id": "YekiIV2RjOlZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "56278712-77bb-4432-de36-7a4b43cba10e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "bacc_rf = metrics.balanced_accuracy_score(y_test, rf_predictions)\n",
    "print(f\"Balanced accuracy: {bacc_rf:0.4f}\" )"
   ],
   "metadata": {
    "id": "9NOWB5qP5vh1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e7ad7f55-4c9b-48cb-f4a0-c5d1c52d67eb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is not much better than the performance we've had with the BoW model.\n",
    "Fine-tuning of RF might help improve the results.\n",
    "\n",
    "We will explore if another classification algorithm - one often used in text classfication - can lead to better performance."
   ],
   "metadata": {
    "id": "KnI96_RXygB-"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Let6ZAP9H9Mt"
   },
   "source": [
    "### Create and evaluate an SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOxQ3L1-H9Mv"
   },
   "source": [
    "This time, we will use a Support Vector Machines (SVM) classifier. This type of classifier has proven successful in text classification tasks; it is generally known to deal well with high dimensional data (as is the case here). It also works well with (relatively) small datasets.\n",
    "For a quick introduction to SVM, watch this short [video tutorial](https://www.youtube.com/watch?v=N1vOgolbjSc).\n",
    "\n",
    "We will use the simplest version of this classifier, which is SVM with linear kernel (that is, linear transformation of the original features). This type of classifier is implemented in sklearn's [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) class"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cROnstwFH9Mv",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "outputId": "99b29e87-5e53-49c2-88f2-d2a7caf4ec37"
   },
   "source": [
    "svm_classifier = SVC(kernel='linear',\n",
    "                     C=1, # default value of the hiper-parameter\n",
    "                     random_state=RAND_STATE)\n",
    "\n",
    "svm_pipe = Pipeline(\n",
    "    [\n",
    "        ('vectorizer', column_transformer),\n",
    "        ('resampling', smote),\n",
    "        ('classifier', svm_classifier)\n",
    "     ],\n",
    "     verbose=True\n",
    ")\n",
    "\n",
    "svm_pipe.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjNhZEd5H9Mw"
   },
   "source": [
    "Make predictions on the test set and compute evaluation measures"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6dmMmOwCH9Mw",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "outputId": "c62cdc32-2f5a-4549-9f68-5cd090867fa0"
   },
   "source": [
    "ConfusionMatrixDisplay.from_estimator(svm_pipe, X_test, y_test, display_labels=y_levels, cmap='Greens');"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9Y-jxUlTH9My",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3da6d11f-0200-4692-a76d-0bdda00eb4ba"
   },
   "source": [
    "svm_predictions = svm_pipe.predict(X_test)\n",
    "\n",
    "perf_svm = metrics.classification_report(y_test, svm_predictions)\n",
    "print(perf_svm)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4TShLR8nH9M0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "82ef2f17-a37d-41bf-bb4e-20ed5370b819"
   },
   "source": [
    "bacc_svm = metrics.balanced_accuracy_score(y_test, svm_predictions)\n",
    "print(f\"Balanced accuracy: {bacc_svm:0.4f}\" )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Improve the SVM model through hyper-parameter tuning\n",
    "\n",
    "We can try to improve the classification model through hyper-parameter tuning. In particular, we'll examine a range of values for the hyper-parameter **C**. With this hyper-parameter, we specify how much we want to penalize missclassified points - the higher **C** value, the higher the penalty, but also the higher risk of overfitting. We will do the tuning through grid search over the specified range of parameter C values.<br>\n",
    "\n",
    "Note: for GridSearchCV, hyper-paramter names have to be preceded by the name of the classifier as given in the pipeline; in this case, the name is `classifer`, so the hyper-paramter C got prefix `classifier__`."
   ],
   "metadata": {
    "id": "2R7NLd_6SzjO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "param_grid = dict(classifier__C=[0.01, 0.1, 1, 10, 100])\n",
    "grid_search = GridSearchCV(svm_pipe,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='recall_macro', # we want to boost recall\n",
    "                           verbose=2)\n",
    "# grid_search.set_params(estimator__max_iter=5000)\n",
    "grid_search.fit(X_train, y_train)"
   ],
   "metadata": {
    "id": "PcnEWkZwTAw1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "17a31418-c45e-4d03-853e-fe9f557ba5ec"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print('best parameters: ', grid_search.best_params_)\n",
    "print('best scores: ', grid_search.best_score_)"
   ],
   "metadata": {
    "id": "cgA7Wml8Jn0n",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f23777c9-8419-4fca-f141-f8bd0d023426"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Re-train the SVM classifier with the optimal C value"
   ],
   "metadata": {
    "id": "ox2j3kfmTkyi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "best_c = grid_search.best_params_['classifier__C']\n",
    "svm_pipe.set_params(classifier__C=best_c)\n",
    "svm_pipe.fit(X_train,y_train)"
   ],
   "metadata": {
    "id": "B7a-F5PuTdQp",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "outputId": "e33dba64-a26a-40f1-fd63-6c9dbed09a9c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate the optimised model"
   ],
   "metadata": {
    "id": "txPLkQfVTuKv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ConfusionMatrixDisplay.from_estimator(svm_pipe, X_test, y_test, display_labels=y_levels, cmap='Greens');"
   ],
   "metadata": {
    "id": "XckeautBTwnZ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "outputId": "581a299d-6f1e-422d-e7f6-24e50d0e1432"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "svm_opt_predictions = svm_pipe.predict(X_test)\n",
    "\n",
    "perf_svm_opt = metrics.classification_report(y_test, svm_opt_predictions)\n",
    "print(perf_svm_opt)"
   ],
   "metadata": {
    "id": "Rzd7sYdDT7IG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "34118fa3-b73e-49e4-b902-011add90e2cb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "bacc_svm_opt = metrics.balanced_accuracy_score(y_test, svm_opt_predictions)\n",
    "print(f\"Balanced accuracy: {bacc_svm_opt:0.4f}\" )"
   ],
   "metadata": {
    "id": "GfF8HyP3UoYu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "569d22d4-2a6c-4027-fa38-a176ce5367f5"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compare all the models"
   ],
   "metadata": {
    "id": "DHsWkeG4U2ov"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_model_metrics(y_true, y_pred, model_name):\n",
    "\n",
    "    # compute P, R, F1\n",
    "    report = metrics.classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "    # compute balanced accuracy\n",
    "    b_acc = metrics.balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # extract metrics for both macro and weighted averages\n",
    "    macro_avg = report['macro avg']\n",
    "    weighted_avg = report['weighted avg']\n",
    "\n",
    "    # Create the results dictionary\n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Macro Precision': macro_avg['precision'],\n",
    "        'Macro Recall': macro_avg['recall'],\n",
    "        'Macro F1-Score': macro_avg['f1-score'],\n",
    "        'Weighted Precision': weighted_avg['precision'],\n",
    "        'Weighted Recall': weighted_avg['recall'],\n",
    "        'Weighted F1-Score': weighted_avg['f1-score'],\n",
    "        'Balanced Accuracy': b_acc\n",
    "    }\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "id": "6MKndMgG1y8Z"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create also a dummy classifier to serve as the general baseline."
   ],
   "metadata": {
    "id": "8oejYI8I2qYx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy='stratified') # generates predictions by respecting the training set’s class distribution\n",
    "\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_pred = dummy_clf.predict(X_test)"
   ],
   "metadata": {
    "id": "3vLOiDMx2cHq"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model_predictions = {\n",
    "    'W2V_RF': rf_predictions,\n",
    "    'W2V_LinSVM': svm_predictions,\n",
    "    'W2V_OptLinSVM': svm_opt_predictions,\n",
    "    'DUMMY': dummy_pred\n",
    "}\n",
    "\n",
    "all_results = list()\n",
    "\n",
    "for model_name, model_pred in model_predictions.items():\n",
    "  model_res = get_model_metrics(y_test, model_pred, model_name)\n",
    "  all_results.append(model_res)\n",
    "\n",
    "all_results_df = pd.DataFrame(all_results)\n",
    "all_results_df.set_index(\"Model\", inplace=True)\n",
    "all_results_df = all_results_df.apply(lambda x: round(x, 4))\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(all_results_df)"
   ],
   "metadata": {
    "id": "TydouGb213wp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9cecf870-d3e0-486c-bee0-3162f06f9fff"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
