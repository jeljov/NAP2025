{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeljov/NAP2025/blob/main/LangChain_Intro.ipynb)"
  },
  {
   "metadata": {
    "id": "AWGzucuFfbBn"
   },
   "cell_type": "markdown",
   "source": [
    "# Introduction to LangChain\n",
    "\n",
    "[LangChain](https://www.langchain.com/) is a popular framework that allows users to quickly build apps and pipelines around **L**arge **L**anguage **M**odels (LLMs). It can be used for chatbots, **G**enerative **Q**uestion-**A**nwering (GQA), summarization, and much more.\n",
    "\n",
    "The core idea of the framework is to allow for _\"chaining\"_ together different components to support advanced use-cases around LLMs (see an illustration [here](https://miro.medium.com/v2/resize:fit:1400/1*jJE0uZTBadEYqe0hEXlWuQ.png)). Chains may consist of multiple components from several modules among which the main are the following:\n",
    "\n",
    "* **Models**: Modern LLMs are typically accessed through a chat model interface that takes a list of messages as input and returns a message as output. The latest generation of chat models natively support **tool calling APIs**, which enable LLMs to interact with external services, APIs, and databases.\n",
    "\n",
    "* **Prompt templates**: Prompt template is an object that takes user input and combines it with a template for a particular prompt type into the final string or message (prompt). There are different type of prompt templates, suitable for different kinds of models and different tasks.\n",
    "\n",
    "* **Output parsers**: These are responsible for taking in the output of a model (strings or a message) and transforming it into a more usable form, such as JSON that matches a given schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r-ryCeG_f_GC"
   },
   "source": [
    "!pip -q install langchain"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNaXrEPOhbuL"
   },
   "source": [
    "# Using large language models in LangChain\n",
    "\n",
    "LangChain supports several large language model (LLM) providers, including both those that offer open and free models (such as Hugging Face) and those that offer proprietary models that require payment (e.g., OpenAI). The use of these different models in LangChain is almost identical, one only needs to instantiate a differnt class to use different models (e.g., ChatGroq vs ChatOpenAI).\n",
    "\n",
    "We will explore the use of LangChain with open LLMs and all we do can be easily mapped to the work with proprietary models."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Access to open models via Groq\n",
    "\n",
    "We will use [Groq](https://groq.com/) API to access a state-of-the-art open model, namely Meta's **LLama 3.1 8B** model. It is a small model in LLama 3.1 group of models and thus not as powerful as its larger 'cousins' (with 70B or 405B parameters), but for the introductory examples it should be fine. Model details can be found [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md).\n",
    "\n",
    "Groq is a company specialised in accelerating the inference process for pre-trained AI models, such as transformer-based models, using specialized hardware and software. It is providing a platform that allows for fast and  efficient running of AI models. It also offers certain number of calls free of charge. To learn about the number of free calls (not worth stating it here as it is prone to change), take a look at [this page](https://console.groq.com/docs/rate-limits). You can sign up [here](https://console.groq.com/keys) for an (free) API key, required to use Groq's API.\n",
    "\n",
    "My Groq API key is stored in the **Colab Secrets**, which is a recommended way of securely storing access tokens and API keys. To learn how to do that and how then to access API tokens / keys stored as Secrets, see, for example, [this short article](https://labs.thinktecture.com/secrets-in-google-colab-the-new-way-to-protect-api-keys/)."
   ],
   "metadata": {
    "id": "OYlAiVgZUgSV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')"
   ],
   "metadata": {
    "id": "wM0DNrB9-5aL"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "GroqAPI can be accessed via LangChain, which allows for easy integration of an LLM into an overall application workflow. To make use of this integration, we first need to install the `langchain_groq` library:"
   ],
   "metadata": {
    "id": "P7GxvSR--1nx"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LWA15ZkVjg80",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "83cfea6c-010d-4546-f9c7-887df6700e7d"
   },
   "source": [
    "!pip -q install langchain_groq"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set up a LLama model and create simple chains\n",
    "\n",
    "Next we create a LLama 3.1 based inference engine, via Groq, by instantiating the ChatGroq class with a model supported by Groq (see the full list [here](https://console.groq.com/docs/models)):"
   ],
   "metadata": {
    "id": "3WADXpIptruQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "creative_llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.9, # setting high temperature to \"foster\" creativity of the model\n",
    "    )"
   ],
   "metadata": {
    "id": "-Nljbk4_q2zu"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "See the [ChatGroq API](https://reference.langchain.com/python/integrations/langchain_groq/?h=chatgroq#langchain_groq.ChatGroq) for the details on the constructor call, the available methods and the response formats"
   ],
   "metadata": {
    "id": "FYRIz37JuRbP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that we are using a Llama model that was fine-tuned for chat, which means that the interaction with the model is typically structured as follows:\n",
    "```\n",
    "System message : You are a helpful and kind assistant that helps users make their travel plans.\n",
    "Your suggestions are concise and to the point.\n",
    "\n",
    "Human: I like big bustling cities, where should I go?\n",
    "\n",
    "AI: You should go to Sydney, Australia\n",
    "\n",
    "Human: Sounds good. What should I do when I'm there?\n",
    "\n",
    "AI:\n",
    "```"
   ],
   "metadata": {
    "id": "l-SwlE3D_8S9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while never including any harmful, unethical, or illegal content.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of giving an incorrect answer.\n",
    "If you don't know the answer to a question, just say \"I do not know\".\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "3SAJ7uCHpe0u"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", DEFAULT_SYSTEM_PROMPT),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "id": "Y0hW1bvIMblb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "cookie_name_question = \"What would be a good name for a company that makes and sells hand-made healthy cookies?\"\n",
    "\n",
    "print(prompt_template.format(question=cookie_name_question))"
   ],
   "metadata": {
    "id": "ZEdEZhn0mtNX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d803f068-ddad-4c3c-874f-b2e90633b13a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "creative_question_chain = prompt_template | creative_llm\n",
    "\n",
    "response = creative_question_chain.invoke({\"question\": cookie_name_question})"
   ],
   "metadata": {
    "id": "KMWAj2iIws-M"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "type(response)"
   ],
   "metadata": {
    "id": "eUyuKXdbC3wr",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "outputId": "604943b9-4351-4906-ce83-098692e0f948"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's explore the response in more detail"
   ],
   "metadata": {
    "id": "86IYa_Bwvg9v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(response.content)"
   ],
   "metadata": {
    "id": "db1LUH1Kvkvg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "504db58f-b8ca-4374-86cf-b14c4c39ae74"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response.response_metadata)"
   ],
   "metadata": {
    "id": "psdlrqpHvxHC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ead3406f-06f6-4119-9510-9037ea5fbe0b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "While not all of these metadata elements are relevant to us now, it would be useful to keep track of the tokens exchanged, as that is directly related to the pricing, that is, the use of the free quota."
   ],
   "metadata": {
    "id": "5G98cWCEyPaX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_tokens_used(groq_response):\n",
    "  token_usage = groq_response.response_metadata['token_usage']\n",
    "  return {\n",
    "      'prompt_tokens':token_usage['prompt_tokens'],\n",
    "      'completion_tokens':token_usage['completion_tokens'],\n",
    "      'total_tokens':token_usage['total_tokens']\n",
    "  }\n",
    "\n",
    "get_tokens_used(response)"
   ],
   "metadata": {
    "id": "GJK-3LUZykcf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cfdfd5e9-6ca1-4166-c0c0-35e1ac06a169"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To make the interaction with an LLM more specific to the task at hand, we can alter the system prompt and provide more task-specific guidance in the user message:"
   ],
   "metadata": {
    "id": "28BCeqpqw7_Z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "branding_system_msg = \"\"\"\n",
    "You are a highly creative assistant, with a plenty of original ideas. You especially excell in inventing catchy brand and product promotional messages.\n",
    "\"\"\"\n",
    "\n",
    "branding_question = \"\"\"\n",
    "What would be a good name for a company that makes and sells {product}? Suggest three distinct names.\n",
    "Structure the results as a python dictionary with the company name as the key and an explanation for the suggested name as the value.\n",
    "Return only this dictionary and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "branding_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", branding_system_msg),\n",
    "        (\"human\", branding_question),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "id": "y79U6VUSOtM_"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build a simple chain using the above created prompt template and LLM"
   ],
   "metadata": {
    "id": "tnen8aKSJHBG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "branding_chain = branding_prompt_template | creative_llm\n",
    "\n",
    "branding_response = branding_chain.invoke({'product': 'colorful running gear'})"
   ],
   "metadata": {
    "id": "5PJk7StNOwZR"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "l7yubiSJhIfs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f5c8f8d5-2c61-4dcd-ee6c-6228b5a7e9dd"
   },
   "source": [
    "print(branding_response.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "get_tokens_used(branding_response)"
   ],
   "metadata": {
    "id": "kG15aSqoyAlZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c1c520f6-4392-4be0-a07e-2f1689df9ad7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also request from the LLM to generate the output in the JSON format and then use LangChain's `JsonOutputParser` to parse the content:"
   ],
   "metadata": {
    "id": "BksC1pj25FM9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "branding_question_json = \"\"\"\n",
    "What would be a good name for a company that makes and sells {product}? Suggest three distinct names.\n",
    "Structure the results as a JSON list of dictionaries, where each dictionary has the company name as its key and an explanation for the suggested name as its value.\n",
    "Return only this JSON list and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "branding_prompt_template_json = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", branding_system_msg),\n",
    "        (\"human\", branding_question_json),\n",
    "    ]\n",
    ")\n",
    "\n",
    "branding_chain_json = branding_prompt_template_json | creative_llm | JsonOutputParser()"
   ],
   "metadata": {
    "id": "WqMz8RJE4VPJ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "branding_json = branding_chain_json.invoke({'product': 'colorful running gear'})"
   ],
   "metadata": {
    "id": "825h3THa4xev"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(branding_json)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZhgKv4bb45pO",
    "outputId": "5440cad3-2855-46b4-d915-f0e7ce2ce7a1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "type(branding_json)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpG0GxVMNHz2",
    "outputId": "317d9177-69f1-4fda-903b-2c89325cee9f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simple chain sequencing\n",
    "\n",
    "We can also explore a simple sequantial chain:"
   ],
   "metadata": {
    "id": "uVpezcDBoaBP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Chain 1\n",
    "chain1_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", branding_system_msg),\n",
    "    (\"human\", \"Please suggest a catchy name for a company that makes and sells {product}. Return just the company name and nothing else.\")\n",
    "])\n",
    "\n",
    "chain1 = chain1_prompt | creative_llm | StrOutputParser()"
   ],
   "metadata": {
    "id": "nwB5qmbAojla"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Chain 2\n",
    "chain2_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", branding_system_msg),\n",
    "    (\"human\", \"Please write a short promotional message for the following company: {company_name}.\")\n",
    "])\n",
    "\n",
    "chain_seq = {\"company_name\": chain1} | chain2_prompt | creative_llm | StrOutputParser()\n",
    "\n",
    "company_desc = chain_seq.invoke({\"product\":\"hand-made healthy cookies\"})"
   ],
   "metadata": {
    "id": "2O6qV1azpEP_"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that in the `chain_seq` code above, the dictionary `{\"company_name\": chain1}` should be interpreted as the output of `chain1` is stored in the variable `company_name`"
   ],
   "metadata": {
    "id": "6aJsaNbKldpD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(company_desc)"
   ],
   "metadata": {
    "id": "ErTQt2EKX31m",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6105f301-9509-4e1c-a0a9-154f32449e6e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zoxlXHYLQix"
   },
   "source": [
    "### Answering factual questions\n",
    "\n",
    "We can also ask the LLM some factual questions. But, for that, we will need a new model instance, since to answer factual questions the model's playfulness (temperature) needs to be reduced to zero\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "factual_llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\", #\"llama-3.1-8b-instant\"\n",
    "    temperature=0\n",
    ")"
   ],
   "metadata": {
    "id": "dWHDwe36hBA-"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "questions = [\n",
    "    \"What is the capital of Serbia?\",\n",
    "    \"What is the largest city in Europe in terms of population?\",\n",
    "    \"What movie got the highest number of the U.S. Academy Awards (aka Oscars)?\",\n",
    "    \"Where the next Winter Olympic Games will be held?\"\n",
    "]"
   ],
   "metadata": {
    "id": "N27jXP6v3Vyd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "factual_questions_system_msg = \"\"\"\n",
    "Please answer the user's questions by giving just a direct, factual response and include a short explanation for each response.\n",
    "If you do not know the answer, respond with 'I don't know'.\n",
    "\"\"\"\n",
    "\n",
    "questions_str = \"\\n\".join([f\"{i+1}) {q}\" for i, q in enumerate(questions)])"
   ],
   "metadata": {
    "id": "KccKbMs13eBg"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "factual_questions_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", factual_questions_system_msg),\n",
    "    (\"human\", \"QUESTIONS:\\n{questions}\")\n",
    "])\n",
    "\n",
    "print(factual_questions_prompt.format(questions=questions_str))"
   ],
   "metadata": {
    "id": "IT7Wy07xhtBX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "aadaf160-ead9-4c5f-d186-f5af775cfc80"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "factual_questions_chain = factual_questions_prompt | factual_llm\n",
    "\n",
    "factual_responses = factual_questions_chain.invoke({'questions': questions_str})"
   ],
   "metadata": {
    "id": "z-k8e8dTBdfe"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(factual_responses.content)"
   ],
   "metadata": {
    "id": "NBSZE-5oB2-W",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "73a21047-639e-431e-827b-461958fcee8d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "get_tokens_used(factual_responses)"
   ],
   "metadata": {
    "id": "kzyVKJkwQtl8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "314f89df-c13e-4c52-94e9-fe5ccab44391"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now challenge the LLM with questions it should not be able to answer:"
   ],
   "metadata": {
    "id": "F-KmyfYxRqEj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "factual_responses2 = factual_questions_chain.invoke({'questions': \"Who is the current president of the USA?\\nWho is currently the best tennis player accoridng to the ATP list?\"})\n",
    "print(factual_responses2.content)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LX5bNyOHRJZA",
    "outputId": "02c48244-b214-417a-b1d8-2d18ccffc925"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpdXG9YtzrLJ"
   },
   "source": [
    "### Instantiating and using proprietary LLMs: an OpenAI example\n",
    "\n",
    "Let's try the same tasks with OpenAI's models. LangChain offers extensive support for working with OpenAI models, available through the `langchain_openai` package. For an overview, check [this page of the LangChain documentation](https://python.langchain.com/docs/integrations/providers/openai/).\n",
    "\n",
    "We'll start by installing additional libraries and setting up a few additional prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FHo2YRHPDgHH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0fd61f00-942c-4d9b-d41d-ee337774f0ca"
   },
   "source": [
    "!pip install -q langchain-core openai langchain-openai"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fOo9qQvDgkz"
   },
   "source": [
    "To use OpenAI's generative models, we need to get our API key which can be retrieved by signing up for an account on the\n",
    "[OpenAI's page for developers](https://platform.openai.com/docs/overview).\n",
    "\n",
    "**Note**: Obtaining an OpenAI API key is free of charge. It is running of their models that is charged, so you can open an account and obtain the API key for free. However, you won't be able to run a model (or any LangChain component that integrates an OpenAI model) until you deposit some money on your OpenAI account. You may also want to check their [pricing sheme](https://platform.openai.com/docs/pricing) to get a better undersanding of how they charge for API calls.\n",
    "\n",
    "Once you have your OpenAI API key, you can add it to the Colab Secrets or enter it in some other privacy protected way. Here, we use Colab Secrets to retrieve API key:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "deWmOJecfbBr"
   },
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AWnaTCP0Ryg"
   },
   "source": [
    "As above, we will access OpenAI's models via LangChain's ChatModel abstraction, since chat (= back and forth interaction with the user) is the interaction mode typical for the latest generation of LLMs. More precisely, this means that we will be using the [`ChatOpenAI` class](https://python.langchain.com/docs/integrations/chat/openai/) from the `langchain_openai` module.\n",
    "\n",
    "As for the specific OpenAI model, we will use **gpt-4o-mini** since we will be working with text only and that model has the best price / performance ratio (as can be seen [here](https://platform.openai.com/docs/pricing))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZhQSDoYe0ly4"
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_creative = ChatOpenAI(model='gpt-4o-mini', temperature=0.9)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGL2zs3uEVj6"
   },
   "source": [
    "We'll use the same simple questions as before with the LLama 3.1 example:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gVSsC3iGEPAp"
   },
   "source": [
    "branding_system_msg = \"\"\"You are a highly creative assistant, with a plenty of original ideas. You especially excell in inventing catchy brand and product promotional messages.\"\"\"\n",
    "branding_question = \"\"\"What would be a good name for a company that makes and sells {product}? Suggest three distinct names.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    (\"system\", branding_system_msg),\n",
    "    (\"human\", branding_question)\n",
    "]\n",
    "\n",
    "\n",
    "branding_prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "branding_openai_chain = branding_prompt_template | openai_creative\n",
    "\n",
    "branding_openai_response = branding_openai_chain.invoke({'product':'colorful running gear'})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "type(branding_openai_response)"
   ],
   "metadata": {
    "id": "0eI2ysdbJupK",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "outputId": "59dd2ac4-f1f8-4a0f-bb22-658f8a1e4381"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The response of OpenAI's GPT-4o-mini model has the same structure as the one we saw above for LLama 3.1 model, that is, it includes the `content` and `response_metadata` components. The difference is in the metadata component, which is more complex in the case of GPT-4o-mini as it can work with multiple modalities."
   ],
   "metadata": {
    "id": "_S_hQna-PT-Y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(branding_openai_response.content)"
   ],
   "metadata": {
    "id": "LelBYqdHMwKK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "50114677-7a64-4a8d-d7ae-498ed659989d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(branding_openai_response.response_metadata)"
   ],
   "metadata": {
    "id": "dTEABGdQ_YHJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3a5576e7-e504-44b6-9229-12f7d21827ce"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRa7Sr8VD0yf"
   },
   "source": [
    "Let's now see how GPT-4o-mini will respond to factual questions.\n",
    "For this task, we will set its temperature to zero, thus decreasing the chance for hallucinations and allowing for replicating the results."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "openai_factual = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0\n",
    ")"
   ],
   "metadata": {
    "id": "66jX-Co2mo33"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b96WIvouLQ-7"
   },
   "source": [
    "questions = [\n",
    "    \"What is the capital of Serbia?\",\n",
    "    \"What is the largest city in Europe in terms of population?\",\n",
    "    \"What movie got the highest number of the U.S. Academy Awards (aka Oscars)?\",\n",
    "    \"Where the next Winter Olympic Games will be held?\",\n",
    "    \"What is the longest commercial flight and between which cities?\"\n",
    "]\n",
    "\n",
    "questions_str = \"\\n\".join([f\"{i+1}) {q}\" for i, q in enumerate(questions)])\n",
    "\n",
    "factual_system_msg = \"\"\"\n",
    "Please answer the questions given below, by giving a direct response followed by a short explanation. If you do not know the answer, respond with 'I don't know'.\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "factual_questions_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", factual_system_msg),\n",
    "    (\"human\", \"QUESTIONS:\\n{questions}\")\n",
    "])\n",
    "\n",
    "factual_openai_chain = factual_questions_prompt | openai_factual | StrOutputParser()\n",
    "\n",
    "factual_openai_responses = factual_openai_chain.invoke({\"questions\":questions_str})"
   ],
   "metadata": {
    "id": "N8qlihX9AzR9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(factual_openai_responses)"
   ],
   "metadata": {
    "id": "qiU426XyDRvz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b3e4679b-532b-4892-be30-cb4e9062dda4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "difficult_factual = factual_openai_chain.invoke({'questions': \"Who is the current president of the USA?\\nWho is currently the best tennis player accoridng to the ATP list?\"})\n",
    "print(difficult_factual)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tIlgIAn5TldN",
    "outputId": "e4563df0-e76d-4c20-ee26-587de9c552aa"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Choosing an LLM\n",
    "\n",
    "The pace of large language model development and deprecation is extremely fast. Furthermore, there are so many different LLMs, both open and proprietary ones. So, a question that naturally arises is how to choose a model to use in a particular LLM-based application.\n",
    "\n",
    "While there is no easy and straightforward answer to that question, what one can do is to:\n",
    "* The [Chatbot Arena](https://lmarena.ai/leaderboard/) is another well known leaderboard that unlike the previously mentioned one includes both open and proprietary models\n",
    "* Vellum.ai maintains [a few leaderboards](https://www.vellum.ai/llm-leaderboard) - in addition to the main one, there is one for open models, one for coding models, as well as an option for direct comparison of two selected models\n",
    "* Keep track of models offered by [OpenAI](https://platform.openai.com/docs/models/overview) and [Google](https://deepmind.google/models/), as the major providers of proprietary models\n",
    "* read online available articles (from credible sources) offering a comparison of current LLMs"
   ],
   "metadata": {
    "id": "L1eGnRw06t9U"
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12 (main, Apr  5 2022, 01:52:34) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
