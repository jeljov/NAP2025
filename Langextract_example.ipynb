{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Using LangExtract to extract entities and their relations\n",
    "\n",
    "[LangExtract](https://github.com/google/langextract/) is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions, in particular, a user-defined prompt and a few examples illustrating the kind of information that is to be extracted.\n",
    "\n",
    "Like the previous information extraction (IE) approach we explored (in week 7), this one is also based on the user's instructions (prompt). However, unlike the previous IE approach that was based on (annotated) data schema, this approach relies on an example (one or more) of the data to be extracted."
   ],
   "metadata": {
    "id": "9ErXivtbU5Qk"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will explore LangExtract's information extraction approach on an example of extracting information about tech companies, their software products and their key employees, from the text of a news article."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T16:01:33.586827Z",
     "start_time": "2025-12-25T16:01:31.197280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import textwrap\n",
    "import langextract as lx\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from collections import defaultdict\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 1: Define a concise prompt with instructions what information should be extracted"
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = textwrap.dedent(\"\"\"\n",
    "    You are highly experienced in extracting named entities and their relations from text, especially from news articles and similar kinds of textual content.\n",
    "\n",
    "    Your task is to extract people, companies, and software products mentioned in the text given below.\n",
    "\n",
    "    Provide meaningful attributes for each entity you identify, to establish connections between entities of different types. For example, to establish a connection between a person and a company or a company and a software product.\n",
    "\n",
    "    Important: Use exact text from the input for extraction text. Do not paraphrase.\n",
    "    Extract entities in order of their appearance with no overlapping text spans.\n",
    "\"\"\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YxRjCgcWUA4o",
    "outputId": "1def9109-6a11-4e81-ad10-c7986de4b688",
    "ExecuteTime": {
     "end_time": "2025-12-25T16:01:34.885670Z",
     "start_time": "2025-12-25T16:01:34.882457Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 2: Provide a high-quality example to guide the model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T16:05:33.024634Z",
     "start_time": "2025-12-25T16:05:33.018488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "examples = [\n",
    "    lx.data.ExampleData(\n",
    "        text=(\n",
    "            \"Llion Jones, a co-founder of Sakana AI, has recently presented one of the company's key products: an AI-based tool called AI Scientist.\"\n",
    "        ),\n",
    "        extractions=[\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"person\",\n",
    "                extraction_text=\"Llion Jones\",\n",
    "                attributes={\"works_for\": \"Sakana AI\", \"role\": \"co-founder\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"company\",\n",
    "                extraction_text=\"Sakana AI\",\n",
    "                attributes={\"co-founder\": \"Llion Jones\", \"product\":\"AI Scientist\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"software_product\",\n",
    "                extraction_text=\"AI Scientist\",\n",
    "                attributes={\"developed_by\": \"Sakana AI\"},\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Step 3: Prepare text to be used for IE\n",
    "\n",
    "We will try out the LangExtract's information extraction approach on a [techcrunch article](https://techcrunch.com/2025/08/03/inside-openais-quest-to-make-ai-do-anything-for-you/) about LLM development inside OpenAI.\n",
    "To that end, we need two additional python libraries:\n",
    "* requests - for pulling the content of the article from the given URL\n",
    "* beautifulsoup - for extracting the main text of the article"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T16:12:12.016240Z",
     "start_time": "2025-12-25T16:12:11.868289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "content = []\n",
    "url = \"https://techcrunch.com/2025/08/03/inside-openais-quest-to-make-ai-do-anything-for-you/\"\n",
    "\n",
    "try:\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    content_elem = soup.find(lambda elem: (elem.name=='div') and elem.has_attr('class') and (elem['class'][0] == 'entry-content'))\n",
    "    for p in content_elem.findAllNext(name='p'):\n",
    "        if p.text:\n",
    "            content.append(p.text)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "input_text = \"\\n\".join(content)\n",
    "\n",
    "# store the content in a local file so that we do not need to pull it from the web each time\n",
    "with open(Path.cwd() / 'data' / 'tech_crunch_article.txt', 'w') as f:\n",
    "    f.write(input_text)\n",
    "\n",
    "# input_text[:200]"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 4: Run the extraction on the target text"
  },
  {
   "cell_type": "code",
   "source": [
    "result = lx.extract(\n",
    "    text_or_documents=input_text,\n",
    "    prompt_description=prompt,\n",
    "    examples=examples,\n",
    "    model_url=\"http://localhost:11434\", # Automatically selects Ollama provider\n",
    "    model_id=\"gemma3:4b\",\n",
    "    fence_output=False,\n",
    "    use_schema_constraints=False,\n",
    "    debug=False,\n",
    "    # extraction_passes=2 # Number of sequential extraction attempts to improve recall by finding additional entities\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96A2tS-VXnEy",
    "outputId": "106ddf3d-128e-46ca-f1a7-2abe81bc9494",
    "ExecuteTime": {
     "end_time": "2025-12-25T16:23:26.719252Z",
     "start_time": "2025-12-25T16:20:14.811254Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note: in the above call of the `extract` function, we set `fence_output=False` and `use_schema_constraints=False` because LangExtract doesn't (yet) implement schema constraints for models other than Gemini. That is, only for Gemini models, it can be requested that the produced output is fully aligned wit the given schema.\n",
    "\n",
    "For more details about the `extract` function, check the source code, available [here](https://github.com/google/langextract/blob/main/langextract/extraction.py)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We will now explore the results.\n",
    "\n",
    "The results are stored in an instance of [`AnnotatedDocument` class](https://github.com/google/langextract/blob/main/langextract/core/data.py#L184), and the extracted data is in its `extractions` list attribute:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T16:23:41.344187Z",
     "start_time": "2025-12-25T16:23:41.338656Z"
    }
   },
   "cell_type": "code",
   "source": "len(result.extractions)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T16:36:43.771450Z",
     "start_time": "2025-12-25T16:36:43.765462Z"
    }
   },
   "cell_type": "code",
   "source": "print(result.extractions[0])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction(extraction_class='person', extraction_text='Hunter Lightman', char_interval=CharInterval(start_pos=14, end_pos=29), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=1, group_index=0, description=None, attributes={'works_for': 'OpenAI', 'role': 'researcher'})\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T16:39:40.730385Z",
     "start_time": "2025-12-25T16:39:40.723322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "people = defaultdict(list)\n",
    "companies = defaultdict(list)\n",
    "products = defaultdict(list)\n",
    "other = defaultdict(list)\n",
    "\n",
    "for extr_item in result.extractions:\n",
    "\n",
    "    ext_type = extr_item.extraction_class\n",
    "    ext_lbl = extr_item.extraction_text\n",
    "\n",
    "    if ext_type == \"person\":\n",
    "        people[ext_lbl.lower()].append(extr_item)\n",
    "    elif ext_type == \"company\":\n",
    "        companies[ext_lbl.lower()].append(extr_item)\n",
    "    elif ext_type  == 'software_product':\n",
    "        products[ext_lbl.lower()].append(extr_item)\n",
    "    else: other[ext_lbl.lower()].append(extr_item)\n",
    "\n",
    "print(f\"Number of unique persons: {len(people)}\")\n",
    "print(f\"Number of unique companies: {len(companies)}\")\n",
    "print(f\"Number of unique software products: {len(products)}\")\n",
    "print(f\"Number of other unique entities: {len(other)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique persons: 13\n",
      "Number of unique companies: 8\n",
      "Number of unique software products: 20\n",
      "Number of other unique entities: 0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "list(other.items())[0]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T16:41:43.928237Z",
     "start_time": "2025-12-25T16:41:43.924633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_extracted_entities(entity_type:str, entities:dict):\n",
    "    print(f\"Extracted entities of type {entity_type.upper()}:\")\n",
    "    for entity, entity_occurrences in entities.items():\n",
    "        for eo in entity_occurrences:\n",
    "            position_info = \"\"\n",
    "            if eo.char_interval:\n",
    "                start, end = eo.char_interval.start_pos, eo.char_interval.end_pos\n",
    "                position_info = f\" (pos: {start}-{end})\"\n",
    "            print(f\"• {eo.extraction_text} {position_info}: {eo.attributes if eo.attributes else 'no attr.'}\")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T16:41:45.978379Z",
     "start_time": "2025-12-25T16:41:45.975270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print_extracted_entities(dict(people))\n",
    "print_extracted_entities(\"person\", people)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted entities of type PERSON:\n",
      "• Hunter Lightman  (pos: 14-29): {'works_for': 'OpenAI', 'role': 'researcher'}\n",
      "• Sam Altman  (pos: 1520-1530): {'title': 'CEO', 'works_for': 'OpenAI'}\n",
      "• Mark Zuckerberg  (pos: 2008-2023): {'recruited': 'Shengjia Zhao'}\n",
      "• Shengjia Zhao  (pos: 2187-2200): {'works_for': 'Meta', 'title': 'chief scientist', 'works_at': 'Meta Superintelligence Labs'}\n",
      "• Andrej Karpathy  (pos: 2792-2807): {'works_for': 'OpenAI', 'role': 'first employee'}\n",
      "• Ahmed El-Kishky  (pos: 3783-3798): {'works_for': 'OpenAI', 'role': 'researcher'}\n",
      "• Lightman  (pos: 4296-4304): {'works_for': 'OpenAI', 'role': 'researcher'}\n",
      "• Lightman  (pos: 4743-4751): {'works_for': 'OpenAI'}\n",
      "• Lightman  (pos: 5752-5760): {'works_for': 'OpenAI'}\n",
      "• Lightman  (pos: 6993-7001): {'approach': 'focusing on the model’s results'}\n",
      "• Lightman  (pos: 9281-9289): {'quote': 'Like many problems in machine learning, it’s a data problem” \\x90”said Lightman when asked about the limitations of agents on subjective tasks.” \\x90”'}\n",
      "• Ilya Sutskever  (pos: 5321-5335): {'works_for': 'OpenAI', 'title': 'co-founder', 'role': 'leader'}\n",
      "• Mark Chen  (pos: 5360-5369): {'works_for': 'OpenAI', 'title': 'chief research officer', 'role': 'leader'}\n",
      "• Jakub Pachocki  (pos: 5391-5405): {'works_for': 'OpenAI', 'title': 'chief scientist', 'role': 'leader'}\n",
      "• El-Kishky  (pos: 6763-6772): {'works_for': 'OpenAI', 'field': 'computer science', 'opinion': 'thinks about the concept in terms of'}\n",
      "• El-Kishky  (pos: 10692-10701): {'works_for': 'OpenAI'}\n",
      "• Nathan Lambert  (pos: 7694-7708): {'works_for': 'AI2', 'role': 'AI researcher', 'comparison_subject': 'AI reasoning modes', 'comparison_object': 'airplanes'}\n",
      "• Noam Brown  (pos: 9514-9524): {'works_for': 'OpenAI', 'role': 'researcher'}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T16:43:46.260974Z",
     "start_time": "2025-12-25T16:43:46.258449Z"
    }
   },
   "cell_type": "code",
   "source": "print_extracted_entities(\"company\", companies)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted entities of type COMPANY:\n",
      "• OpenAI  (pos: 37-43): {'employee': 'Hunter Lightman'}\n",
      "• OpenAI  (pos: 889-895): no attr.\n",
      "• OpenAI  (pos: 2921-2927): {'employee': 'Andrej Karpathy'}\n",
      "• OpenAI  (pos: 4071-4077): {'product': 'o1'}\n",
      "• OpenAI  (pos: 4628-4634): {'leader': 'Daniel Selsam', 'leader_title': 'researcher'}\n",
      "• OpenAI  (pos: 5506-5512): {'research_focus': 'AGI'}\n",
      "• OpenAI  (pos: 7449-7455): {'researchers': 'Nathan Lambert'}\n",
      "• OpenAI  (pos: 8426-8432): {'agent': 'Codex agent', 'agent_purpose': 'help software engineers offload simple coding tasks'}\n",
      "• OpenAI  (pos: 8737-8743): {'agent': 'ChatGPT agent', 'agent_purpose': 'struggle with many of the complex, subjective tasks people want to automate'}\n",
      "• OpenAI  (pos: 9529-9535): {'employee': 'Noam Brown', 'product': 'o1'}\n",
      "• OpenAI  (pos: 10363-10369): {'model': 'GPT-5'}\n",
      "• OpenAI  (pos: 11245-11251): {'industry': 'AI'}\n",
      "• Meta  (pos: 2240-2244): {'employee': 'Shengjia Zhao', 'unit': 'superintelligence-focused unit'}\n",
      "• Meta  (pos: 11492-11496): {'competitor': 'OpenAI'}\n",
      "• Google DeepMind  (pos: 2629-2644): {'developed': 'AlphaGo'}\n",
      "• Google DeepMind  (pos: 8115-8130): {'involved_in_paper': True}\n",
      "• AI2  (pos: 7747-7750): {'researcher': 'Nathan Lambert'}\n",
      "• Anthropic  (pos: 8100-8109): {'involved_in_paper': True}\n",
      "• Anthropic  (pos: 8519-8528): {'models': ['Claude Code', 'Cursor'], 'models_role': 'AI coding tools'}\n",
      "• Anthropic  (pos: 11473-11482): {'competitor': 'OpenAI'}\n",
      "• Perplexity  (pos: 8764-8774): {'agent': 'Comet', 'agent_purpose': 'struggle with many of the complex, subjective tasks people want to automate'}\n",
      "• Google  (pos: 10021-10027): {'develops': 'state-of-the-art models'}\n",
      "• Google  (pos: 11465-11471): {'competitor': 'OpenAI'}\n",
      "• xAI  (pos: 10032-10035): {'develops': 'state-of-the-art models'}\n",
      "• xAI  (pos: 11484-11487): {'competitor': 'OpenAI'}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T16:44:48.672502Z",
     "start_time": "2025-12-25T16:44:48.665958Z"
    }
   },
   "cell_type": "code",
   "source": "print_extracted_entities(\"software product\", products)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted entities of type SOFTWARE PRODUCT:\n",
      "• ChatGPT  (pos: 102-109): {'developed_by': 'OpenAI'}\n",
      "• ChatGPT  (pos: 1220-1227): {'developed_by': 'OpenAI', 'status': 'research preview turned viral consumer business'}\n",
      "• ChatGPT  (pos: 3198-3205): {'developed_by': 'OpenAI', 'derived_from': 'GPT series'}\n",
      "• ChatGPT  (pos: 6607-6614): {'UX_features': ['thinking', 'reasoning']}\n",
      "• ChatGPT  (pos: 11007-11014): {'developed_by': 'OpenAI'}\n",
      "• MathGen  (pos: 290-297): {'developed_by': 'OpenAI', 'related_to': 'AI reasoning models'}\n",
      "• o1  (pos: 1843-1845): {'developed_by': 'OpenAI', 'release_date': 'fall of 2024'}\n",
      "• o1  (pos: 4067-4069): {'developed_by': 'OpenAI'}\n",
      "• o1  (pos: 5796-5798): {'developed_by': 'OpenAI'}\n",
      "• o1  (pos: 6603-6605): {'developed_by': 'ChatGPT'}\n",
      "• o1  (pos: 9583-9585): {'developed_by': 'OpenAI'}\n",
      "• OpenAI’s reasoning models and agents  (pos: 2281-2317): {'training_technique': 'reinforcement learning (RL)'}\n",
      "• reinforcement learning (RL)  (pos: 2377-2403): {'used_by': 'OpenAI’s reasoning models and agents', 'used_for': 'provides feedback to an AI model on whether its choices were correct in simulated environments'}\n",
      "• AlphaGo  (pos: 2655-2662): {'developed_by': 'Google DeepMind', 'uses': 'reinforcement learning (RL)', 'achieved': 'gained global attention after beating a world champion in the board game Go'}\n",
      "• GPT series  (pos: 3049-3059): {'developed_by': 'OpenAI', 'pretrained_on': 'massive amounts of internet data and large clusters of GPUs'}\n",
      "• Q*  (pos: 3314-3315): {'developed_by': 'OpenAI', 'combination_of': ['LLMs', 'RL', 'test-time computation']}\n",
      "• Strawberry  (pos: 3328-3338): {'developed_by': 'OpenAI', 'predecessor_of': 'Q*'}\n",
      "• Strawberry  (pos: 4014-4024): {'developed_by': 'OpenAI'}\n",
      "• test-time computation  (pos: 3388-3409): {'used_by': 'Strawberry'}\n",
      "• chain-of-thought  (pos: 3610-3626): no attr.\n",
      "• CoT  (pos: 3629-3632): {'developed_by': 'OpenAI', 'improves': 'ChatGPT'}\n",
      "• Agents  (pos: 4820-4826): {'developed_by': 'OpenAI', 'leader': 'Daniel Selsam'}\n",
      "• o1 reasoning model  (pos: 5260-5278): {'developed_by': 'OpenAI', 'leaders': ['Ilya Sutskever', 'Mark Chen', 'Jakub Pachocki']}\n",
      "• AI reasoning models  (pos: 5983-6002): {'developed_by': 'several leading AI labs'}\n",
      "• AI agents  (pos: 8333-8342): {'best_for': 'well-defined, verifiable domains such as coding'}\n",
      "• Cursor  (pos: 8595-8601): {'developed_by': 'Anthropic'}\n",
      "• Claude Code  (pos: 8606-8617): {'developed_by': 'Anthropic'}\n",
      "• IMO model  (pos: 9569-9578): {'developed_by': 'OpenAI', 'achievement': 'gold medal at IMO'}\n",
      "• GPT-5  (pos: 10454-10459): {'developed_by': 'OpenAI'}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The results can be store in the JSONL (JSON Lines) format, a text-based format where each line represents a valid JSON object; it is used for storing structured data records. For more info, see [https://jsonltools.com/what-is-jsonl](https://jsonltools.com/what-is-jsonl)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T16:46:08.102026Z",
     "start_time": "2025-12-25T16:46:08.092140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_fname = \"tech_companies_IE_results\"\n",
    "\n",
    "lx.io.save_annotated_documents([result],\n",
    "                               output_name=f\"{results_fname}.jsonl\",\n",
    "                               output_dir=Path.cwd() / 'ie_results')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[94m\u001B[1mLangExtract\u001B[0m: Saving to \u001B[92mtech_companies_IE_results.jsonl\u001B[0m: 1 docs [00:00, 334.66 docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92m✓\u001B[0m Saved \u001B[1m1\u001B[0m documents to \u001B[92mtech_companies_IE_results.jsonl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The JSONL file can be used for generating an HTML document with interactive visualization of the results"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T16:47:04.987691Z",
     "start_time": "2025-12-25T16:47:04.974406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_json = f\"{Path.cwd()}/ie_results/{results_fname}.jsonl\"\n",
    "\n",
    "html_content = lx.visualize(results_json)\n",
    "\n",
    "with open(f\"{Path.cwd()}/ie_results/{results_fname}.html\", \"w\") as f:\n",
    "     f.write(html_content.data)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[94m\u001B[1mLangExtract\u001B[0m: Loading \u001B[92mtech_companies_IE_results.jsonl\u001B[0m: 100%|██████████| 32.7k/32.7k [00:00<00:00, 16.7MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92m✓\u001B[0m Loaded \u001B[1m1\u001B[0m documents from \u001B[92mtech_companies_IE_results.jsonl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Identify relationships between entities, as they can offer a solid ground for creating a knowledge graph"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "relations = defaultdict(list)\n",
    "\n",
    "for extr_item in result.extractions:\n",
    "\n",
    "    ext_lbl = extr_item.extraction_text\n",
    "    if extr_item.attributes:\n",
    "        for ext_attr in extr_item.attributes.items():\n",
    "            attr_rel, attr_val = ext_attr\n",
    "            relations[attr_rel].append((ext_lbl, attr_val))\n",
    "\n",
    "\n",
    "for rel in relations.keys():\n",
    "    print(rel)\n",
    "    for pair in relations[rel]:\n",
    "        print(pair)\n",
    "    print(\"-------------------\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# list the distinct kinds of relations\n",
    "relations.keys()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Some of these relations can / should be merged (e.g., 'works_for' and 'works_at', or 'employee' and 'employs'), to better capture the semantics of relations and set the grounds for knowledge graph creation. [This YouTube video](https://www.youtube.com/watch?v=dPL2vRDunMw) shows how that can be done."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Explore LangExtract further\n",
    "\n",
    "To get a better understanding of LangExtract features and how it can be used for information extraction, it is recommended to explore:\n",
    "* the examples available at the LangExtract's GitHub repo and especially the [Medication extraction example](https://github.com/google/langextract/blob/main/docs/examples/medication_examples.md)\n",
    "* the examples presented in the [DataCamp's LangExtract tutorial](https://www.datacamp.com/tutorial/langextract)\n",
    "\n",
    "#### Compare and contract LangExtract w/ alternative approaches to information extraction\n",
    "\n",
    "A very nice comparison is given in the aforementioned [DataCamp's tutorial](https://www.datacamp.com/tutorial/langextract), specifically in the table towards the end of the article"
   ]
  }
 ]
}
