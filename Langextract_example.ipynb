{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Using LangExtract to extract entities and their relations\n",
    "\n",
    "[LangExtract](https://github.com/google/langextract/) is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions, in particular, a user-defined prompt and a few examples illustrating the kind of information that is to be extracted.\n",
    "\n",
    "Like the previous information extraction (IE) approach we explored (in week 7), this one is also based on the user's instructions (prompt). However, unlike the previous IE approach that was based on (annotated) data schema, this approach relies on an example (one or more) of the data to be extracted."
   ],
   "metadata": {
    "id": "9ErXivtbU5Qk"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will explore LangExtract's information extraction approach on an example of extracting information about tech companies, their software products and their key employees, from the text of a news article."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import textwrap\n",
    "import langextract as lx\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from collections import defaultdict\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 1: Define a concise prompt with instructions what information should be extracted"
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = textwrap.dedent(\"\"\"\n",
    "    You are highly experienced in extracting named entities and their relations from text, especially from news articles and similar kinds of textual content.\n",
    "\n",
    "    Your task is to extract people, companies, and software products mentioned in the text given below.\n",
    "\n",
    "    Provide meaningful attributes for each entity you identify, to establish connections between entities of different types. For example, to establish a connection between a person and a company or a company and a software product.\n",
    "\n",
    "    Important: Use exact text from the input for extraction text. Do not paraphrase.\n",
    "    Extract entities in order of their appearance with no overlapping text spans.\n",
    "\"\"\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YxRjCgcWUA4o",
    "outputId": "1def9109-6a11-4e81-ad10-c7986de4b688"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 2: Provide a high-quality example to guide the model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "examples = [\n",
    "    lx.data.ExampleData(\n",
    "        text=(\n",
    "            \"Llion Jones, a co-founder of Sakana AI, has recently presented one of the company's key products: an AI-based tool called AI Scientist.\"\n",
    "        ),\n",
    "        extractions=[\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"person\",\n",
    "                extraction_text=\"Llion Jones\",\n",
    "                attributes={\"works_for\": \"Sakana AI\", \"role\": \"co-founder\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"company\",\n",
    "                extraction_text=\"Sakana AI\",\n",
    "                attributes={\"co-founder\": \"Llion Jones\", \"product\":\"AI Scientist\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"software_product\",\n",
    "                extraction_text=\"AI Scientist\",\n",
    "                attributes={\"developed_by\": \"Sakana AI\"},\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Step 3: Prepare text to be used for IE\n",
    "\n",
    "We will try out the LangExtract's information extraction approach on a [techcrunch article](https://techcrunch.com/2025/08/03/inside-openais-quest-to-make-ai-do-anything-for-you/) about LLM development inside OpenAI.\n",
    "To that end, we need two additional python libraries:\n",
    "* requests - for pulling the content of the article from the given URL\n",
    "* beautifulsoup - for extracting the main text of the article"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "content = []\n",
    "url = \"https://techcrunch.com/2025/08/03/inside-openais-quest-to-make-ai-do-anything-for-you/\"\n",
    "\n",
    "try:\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    content_elem = soup.find(lambda elem: (elem.name=='div') and elem.has_attr('class') and (elem['class'][0] == 'entry-content'))\n",
    "    for p in content_elem.findAllNext(name='p'):\n",
    "        if p.text:\n",
    "            content.append(p.text)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "input_text = \"\\n\".join(content)\n",
    "\n",
    "# store the content in a local file so that we do not need to pull it from the web each time\n",
    "with open(Path.cwd() / 'data' / 'tech_crunch_article.txt', 'w') as f:\n",
    "    f.write(input_text)\n",
    "\n",
    "# input_text[:200]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 4: Run the extraction on the target text"
  },
  {
   "cell_type": "code",
   "source": [
    "result = lx.extract(\n",
    "    text_or_documents=input_text,\n",
    "    prompt_description=prompt,\n",
    "    examples=examples,\n",
    "    model_url=\"http://localhost:11434\", # Automatically selects Ollama provider\n",
    "    model_id=\"gemma3:4b\",\n",
    "    fence_output=False,\n",
    "    use_schema_constraints=False,\n",
    "    debug=False,\n",
    "    # extraction_passes=2 # Number of sequential extraction attempts to improve recall by finding additional entities\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96A2tS-VXnEy",
    "outputId": "106ddf3d-128e-46ca-f1a7-2abe81bc9494"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note: in the above call of the `extract` function, we set `fence_output=False` and `use_schema_constraints=False` because LangExtract doesn't (yet) implement schema constraints for models other than Gemini. That is, only for Gemini models, it can be requested that the produced output is fully aligned wit the given schema.\n",
    "\n",
    "For more details about the `extract` function, check the source code, available [here](https://github.com/google/langextract/blob/main/langextract/extraction.py)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We will now explore the results.\n",
    "\n",
    "The results are stored in an instance of [`AnnotatedDocument` class](https://github.com/google/langextract/blob/main/langextract/core/data.py#L184), and the extracted data is in its `extractions` list attribute:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(result.extractions)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(result.extractions[0])",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "people = defaultdict(list)\n",
    "companies = defaultdict(list)\n",
    "products = defaultdict(list)\n",
    "other = defaultdict(list)\n",
    "\n",
    "for extr_item in result.extractions:\n",
    "\n",
    "    ext_type = extr_item.extraction_class\n",
    "    ext_lbl = extr_item.extraction_text\n",
    "\n",
    "    if ext_type == \"person\":\n",
    "        people[ext_lbl.lower()].append(extr_item)\n",
    "    elif ext_type == \"company\":\n",
    "        companies[ext_lbl.lower()].append(extr_item)\n",
    "    elif ext_type  == 'software_product':\n",
    "        products[ext_lbl.lower()].append(extr_item)\n",
    "    else: other[ext_lbl.lower()].append(extr_item)\n",
    "\n",
    "print(f\"Number of unique persons: {len(people)}\")\n",
    "print(f\"Number of unique companies: {len(companies)}\")\n",
    "print(f\"Number of unique software products: {len(products)}\")\n",
    "print(f\"Number of other unique entities: {len(other)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "list(other.items())[0]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def print_extracted_entities(entity_type:str, entities:dict):\n",
    "    print(f\"Extracted entities of type {entity_type.upper()}:\")\n",
    "    for entity, entity_occurrences in entities.items():\n",
    "        for eo in entity_occurrences:\n",
    "            position_info = \"\"\n",
    "            if eo.char_interval:\n",
    "                start, end = eo.char_interval.start_pos, eo.char_interval.end_pos\n",
    "                position_info = f\" (pos: {start}-{end})\"\n",
    "            print(f\"â€¢ {eo.extraction_text} {position_info}: {eo.attributes if eo.attributes else 'no attr.'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print_extracted_entities(dict(people))\n",
    "print_extracted_entities(\"person\", people)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print_extracted_entities(\"company\", companies)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print_extracted_entities(\"software product\", products)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The results can be store in the JSONL (JSON Lines) format, a text-based format where each line represents a valid JSON object; it is used for storing structured data records. For more info, see [https://jsonltools.com/what-is-jsonl](https://jsonltools.com/what-is-jsonl)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_fname = \"tech_companies_IE_results\"\n",
    "\n",
    "lx.io.save_annotated_documents([result],\n",
    "                               output_name=f\"{results_fname}.jsonl\",\n",
    "                               output_dir=Path.cwd() / 'ie_results')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The JSONL file can be used for generating an HTML document with interactive visualization of the results"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_json = f\"{Path.cwd()}/ie_results/{results_fname}.jsonl\"\n",
    "\n",
    "html_content = lx.visualize(results_json)\n",
    "\n",
    "with open(f\"{Path.cwd()}/ie_results/{results_fname}.html\", \"w\") as f:\n",
    "     f.write(html_content.data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Identify relationships between entities, as they can offer a solid ground for creating a knowledge graph"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "relations = defaultdict(list)\n",
    "\n",
    "for extr_item in result.extractions:\n",
    "\n",
    "    ext_lbl = extr_item.extraction_text\n",
    "    if extr_item.attributes:\n",
    "        for ext_attr in extr_item.attributes.items():\n",
    "            attr_rel, attr_val = ext_attr\n",
    "            relations[attr_rel].append((ext_lbl, attr_val))\n",
    "\n",
    "\n",
    "for rel in relations.keys():\n",
    "    print(rel)\n",
    "    for pair in relations[rel]:\n",
    "        print(pair)\n",
    "    print(\"-------------------\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# list the distinct kinds of relations\n",
    "relations.keys()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Some of these relations can / should be merged (e.g., 'works_for' and 'works_at', or 'employee' and 'employs'), to better capture the semantics of relations and set the grounds for knowledge graph creation. [This YouTube video](https://www.youtube.com/watch?v=dPL2vRDunMw) shows how that can be done."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Explore LangExtract further\n",
    "\n",
    "To get a better understanding of LangExtract features and how it can be used for information extraction, it is recommended to explore:\n",
    "* the examples available at the LangExtract's GitHub repo and especially the [Medication extraction example](https://github.com/google/langextract/blob/main/docs/examples/medication_examples.md)\n",
    "* the examples presented in the [DataCamp's LangExtract tutorial](https://www.datacamp.com/tutorial/langextract)\n",
    "\n",
    "#### Compare and contract LangExtract w/ alternative approaches to information extraction\n",
    "\n",
    "A very nice comparison is given in the aforementioned [DataCamp's tutorial](https://www.datacamp.com/tutorial/langextract), specifically in the table towards the end of the article"
   ]
  }
 ]
}
