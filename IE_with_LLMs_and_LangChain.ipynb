{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeljov/NAP2025/blob/main/IE_with_LLMs_and_LangChain.ipynb)"
  },
  {
   "metadata": {
    "id": "QNAAn89Piq0c"
   },
   "cell_type": "markdown",
   "source": "## Information extraction (IE) via LLMs and function calling"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Information extraction** (IE) is often defined as the automated process of identifying and extracting specific, structured pieces of information from unstructured or semi-structured data sources, such as text documents, emails, reviews or medical recods (see [here](https://www.width.ai/post/emr-data-extraction) examples of such documents).\n",
    "The primary goal is to transform human-readable, free-form text into a structured, machine-readable format, such as a database table or a knowledge graph, that computers can easily process and analyze."
   ],
   "metadata": {
    "id": "OZCGbhdxXd77"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Information extraction can be seamlessly done using LLMs that support ***function calling***, also known as ***tool calling***.\n",
    "\n",
    "Tool calling is about providing an LLM with ability to communicate with external tools (APIs, data bases, search engines, etc). It is well explained in [OpenAI's documentation](https://platform.openai.com/docs/guides/function-calling) and the figure below, taken from that documentation, offers a nice illustration of function (tool) calling workflow:\n",
    "\n",
    "<img src=\"https://cdn.openai.com/API/docs/images/function-calling-diagram-steps.png\" width=\"470\" height=\"630\" alt=\"Illustration of function calling workflow\">"
   ],
   "metadata": {
    "id": "T9QUR0v_i-l0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Our task: information extraction from customer reviews\n",
    "\n",
    "We will use tool-calling features of an open-weights LLM available via Groq and LangChain's integration with Groq, to extract structured information from text of customer reviews.\n",
    "\n",
    "Note: LangChain supports tool-calling for several other LLMs. A list of supported LLMs is available, for example, [here](https://python.langchain.com/docs/integrations/chat/).\n",
    "\n",
    "We start with the regular setup steps: installing the required packages and loading the required access tokens."
   ],
   "metadata": {
    "id": "GAP624lbxH6s"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RRYSu48huSUW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1d06df30-2d1b-4235-8c57-ed960db332be"
   },
   "source": [
    "!pip -q install langchain langchain_groq"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from os import environ\n",
    "from google.colab import userdata\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
    "\n",
    "RAND_STATE = 1"
   ],
   "metadata": {
    "id": "dNA4TsHpu6OM"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Schema for information extraction and tagging\n",
    "\n",
    "Information extraction (IE) and tagging in LangChain rely on two key components:\n",
    "\n",
    "* **Schema**: defines what pieces of information we want to extract from the text. For example, we may want to extract the expressed sentiment, the language of the text, people and/or places mentioned in the text, etc.\n",
    "* **Extraction prompt**: instructs the model how to extract information (specified in the schema) from the text; optionally, it may include examples to improve IE through few-shot learning"
   ],
   "metadata": {
    "id": "HqwsGJDhvAQ5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Both the extraction prompt and the schema are passed to an LLM. The LLM does the information extraction, following the instructions from the prompt and the meta-data embedded in the schema. It then creates and returns a JSON object aligned with the schema; LangChain parses the resulting JSON into an instance of the schema python class, as shown on the figure below:\n",
    "<img src=\"https://drive.google.com/uc?id=13rUXdluUE_Xjo-2kCXWr879kblEc79XJ\" alt=\"LLM-based IE Workflow\"/>"
   ],
   "metadata": {
    "id": "fqkny4W-UfD4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function that is called here is the function responsible for the transformation from the Schema (as a python class) to JSON Schema and from a populated JSON Schema (i.e. JSON object) to the corresponding instance (object) of the Schema (python) class."
   ],
   "metadata": {
    "id": "1CMjlOKzH4QT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining the IE schema\n",
    "\n",
    "There are two recommendations (best practices) to follow when defining data schema to be used in the IE task:\n",
    "\n",
    "* *The schema and the attributes it includes should be well documented*. The information contained in the documentation is sent to the LLM and allows for improving the quality of information extraction. See, in the code below, the doc-string describing the AppReview class as well as descriptions associated to each class attribute.\n",
    "\n",
    "* *Do not force the LLM to extract information*; othwerwise, it will make up information when it cannot extract the requested pieces. Below we use `Optional` (from python's `typing` module) when specifying attributes, thus allowing the LLM to output `None` if the requested information is not available or it cannot identify it."
   ],
   "metadata": {
    "id": "hNtGWCXi0Nym"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining data schema in Pydantic\n",
    "\n",
    "[Pydantic](https://docs.pydantic.dev/latest/) is a Python library used for data validation and parsing.\n",
    "In the following we will familiarise with the pydantic elements that are required for defining the schema of the structured output we want an LLM to produce.\n",
    "\n",
    "`BaseModel`\n",
    "* This is the core class we inherit from when defining a data schema for the structured output. Any class that inherits from BaseModel is recognized by Pydantic (and LangChain) as a data structure definition.\n",
    "\n",
    "* Why it is relevant?\n",
    "\n",
    "  1) **Type checking / coercion**. It automatically checks the type of the extracted data. If the LLM returns \"25\" for an int field, Pydantic will coerce it to the integer 25. If the LLM returns \"twenty-five\", Pydantic will throw a validation error, which LangChain uses to prompt the LLM to retry.\n",
    "  2) **JSON Schema Generation**. Pydantic can automatically convert your Python class into a standard JSON Schema. This JSON Schema is what LangChain passes to the LLM (via function calling).\n",
    "\n",
    "In case of using such data model in LangChain to define the expected structured output, it is recommended to add a doc string stating the purpose of the data model (schema)."
   ],
   "metadata": {
    "id": "L4G-YeJ65ThT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class AppReview(BaseModel):\n",
    "  \"\"\"Information about a mobile app review.\"\"\"\n",
    "  pass"
   ],
   "metadata": {
    "id": "D2B-pwLM7bri"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, pydantic allows us to define the expected data type for each field and also to specify if certain fields are optional"
   ],
   "metadata": {
    "id": "T6a63-co7oHn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Optional\n",
    "\n",
    "class AppReview(BaseModel):\n",
    "  \"\"\"Information about a mobile app review.\"\"\"\n",
    "\n",
    "  app_features: list[str]\n",
    "  problems: Optional[list[str]]\n",
    "  sentiment: str\n",
    "  dissatisfaction: int"
   ],
   "metadata": {
    "id": "cV0qDeSG71An"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "It also allows us to define meta-data for each field in the schema, which is highly relevant as via meta-data we are communicating additional instructions to the LLM.\n",
    "\n",
    "The `Field` function is used to add metadata to the schema fields. Its key arguments include:\n",
    "* `description`: This is key argument as it tells the LLM exactly what information to extract for this field. A good description is often the difference between a poor and a precise extraction.\n",
    "* `default`: Sets a default value if the field is optional and not present in the input. It is generally recommended to make all fields optional and with a default value, to prevent LLM from halycinating to generate a field value when there is none in the input content.\n",
    "* `examples`: One can provide a list of expected examples for a field, which further guides the LLM on the field's format and content. Examples are espcially useful for specifying date/time formats or any other conventions for respresenting data.\n",
    "\n"
   ],
   "metadata": {
    "id": "_E8JJQO58oKA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pydantic import Field\n",
    "\n",
    "class AppReview(BaseModel):\n",
    "    \"\"\"Information about a mobile app review.\"\"\"\n",
    "\n",
    "    app_features: Optional[list] = Field(\n",
    "        default=[],\n",
    "        description=\"The terms used in the review to describe the mobile app and its features\",\n",
    "        examples=[['well designed', 'very useful']]\n",
    "    )\n",
    "\n",
    "    problems: Optional[list] = Field(\n",
    "        default=[],\n",
    "        description=\"Any problems that the user mentions in the review regarding the app\",\n",
    "        examples=[['blinking screen', 'annoying ads']]\n",
    "    )\n",
    "\n",
    "    sentiment: Optional[str] = Field(\n",
    "        default=\"unknown\",\n",
    "        description=\"The sentiment expressed in the review. It can be one of the following: Positive, Negative, or Neutral\",\n",
    "        examples=['Positive']\n",
    "    )\n",
    "\n",
    "    dissatisfaction: Optional[int] = Field(\n",
    "        default=None,\n",
    "        description=\"How dissatisfied the user is with the app, on the integer scale from 1 to 5 (higher value means higher dissatisfaction).\",\n",
    "        examples=[3]\n",
    "    )"
   ],
   "metadata": {
    "id": "R5EZ7Hh7_a94"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can further advance the data structure by using python enums to specify allowed values for some of the fields, in this case, for the sentiment expressed in the review:"
   ],
   "metadata": {
    "id": "-gEeJE-J0X2K"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Sentiment(Enum):\n",
    "    NEGATIVE = \"Negative\"\n",
    "    NEUTRAL = \"Neutral\"\n",
    "    POSITIVE = \"Positive\"\n",
    "\n",
    "class AppReview(BaseModel):\n",
    "    \"\"\"Information about a mobile app review.\"\"\"\n",
    "\n",
    "    app_features: Optional[list] = Field(\n",
    "        default=[],\n",
    "        description=\"The terms used in the review to describe the mobile app and its features\",\n",
    "        examples=[['well designed', 'very useful']]\n",
    "    )\n",
    "\n",
    "    problems: Optional[list] = Field(\n",
    "        default=[],\n",
    "        description=\"Any problems that the user mentions in the review regarding the app\",\n",
    "        examples=[['blinking screen', 'annoying ads']]\n",
    "    )\n",
    "\n",
    "    sentiment: Optional[Sentiment] = Field(\n",
    "        default=None,\n",
    "        description=\"The sentiment expressed in the review. It can be one of the following: Positive, Negative, or Neutral\",\n",
    "        examples=[Sentiment.POSITIVE]\n",
    "    )\n",
    "\n",
    "    dissatisfaction: Optional[int] = Field(\n",
    "        default=None,\n",
    "        description=\"How dissatisfied the user is with the app, on the integer scale from 1 to 5 (higher value means higher dissatisfaction).\",\n",
    "        examples=[3]\n",
    "    )"
   ],
   "metadata": {
    "id": "_7i35oemw5AV"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To learn more about Pydantic, you may want to start from [this video](https://www.youtube.com/watch?v=XIdQ6gO3Anc), as it offers an excellent introduction to this python library for data typing and validation. Then, [this video tutorial](https://youtu.be/i4jespFbA1c?si=mjKneeBQdlDcQGll) and the accompanying series of blog posts (https://www.bugbytes.io/posts/introduction-to-pydantic/) allow for exploring further pydantic's features for data and code validation.\n"
   ],
   "metadata": {
    "id": "kqVhGPfZXNoP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Back to the original task: the use of LLMs for IE from text\n",
    "\n",
    "Now that we have defined a data schema for the structured output the LLM should produce, we create an extraction prompt. As before, we are creating a prompt for a chat model and thus specify it as a list of (system and user) messages:"
   ],
   "metadata": {
    "id": "somLHjIXMOaq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "ie_system_msg = \"\"\"\n",
    "You are an expert information extraction agent, specialised in extracting information from users' comments and reviews posted on the internet.\n",
    "You extract only relevant information from the text.\n",
    "If you do not know the value of an attribute you've been asked to extract, return None for the attribute's value.\n",
    "\"\"\"\n",
    "\n",
    "ie_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", ie_system_msg),\n",
    "        (\"human\", \"Extract details about a customer review of a mobile app, given below.\\n{text}\"),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "id": "iFKWOcqN6JTe"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we associate the model with the schema and combine the model with the prompt into an IE chain:\n"
   ],
   "metadata": {
    "id": "l3qPjOujuORt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = \"llama-3.1-8b-instant\"\n",
    "llm = ChatGroq(model=model_name, temperature=0.0)"
   ],
   "metadata": {
    "id": "-aKUsPNAWzyW"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "ie_llm = llm.with_structured_output(schema=AppReview) # this is where the LLM is connected w/ the Schema\n",
    "\n",
    "ie_chain = ie_prompt_template | ie_llm"
   ],
   "metadata": {
    "id": "g4Km9BGl6nyl"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: the method `with_structured_output` (above) automates the process of binding the schema to the model and parsing the model's output into the structure defined by the schema."
   ],
   "metadata": {
    "id": "Xh1acxWNNfsc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "reviews = [\n",
    "    \"not sure i feel with all the permissions you have to give this app and then you have to create an account and log in makes me believe they are using this app for getting information. it could be done with only the minable permissions and use your own memory.\",\n",
    "    \"galaxy s2 love this app. very useful. only thing that bugs me is from time to time my screen blacks out and force closes the app when i back out.\",\n",
    "    \"bad. has nothing for end game content and is very very very outdated.\"\n",
    "]\n"
   ],
   "metadata": {
    "id": "DU7V8iq5nxI8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Invoking the model produces structured output aligned with the given schema:"
   ],
   "metadata": {
    "id": "SnLNAECOujRz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for review in reviews:\n",
    "  print(ie_chain.invoke({'text':review}))\n",
    "  print()"
   ],
   "metadata": {
    "id": "pgLjNdak7KIN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "077ae726-fe66-4139-b92a-05f9abe66bd4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using few-shot prompting to improve the IE results\n",
    "\n",
    "We can try to improve the IE results by including some examples in the prompt, that is, by using few-shot prompting. For chat models, as the one we are using, this can take the form of a sequence of pairs, where each pair consists of an input and the response messages, demonstrating desired behaviors. However, creating such examples is not that easy as it is the case with 'regular' few-shot prompting, when tool calling is not involved. Here is why:\n",
    "\n",
    "To produce structured output, the LLM we use depends, under-the-hood, on tool calling. This typically involves the generation of an AI message (one or more) containing tool calls, as well as tool messages (one or more, depending on the number of tools used) containing the results of tool calls. How exactly these messages are generated and in what order depends on the model provider (e.g., OpenAI, Google, Groq, ...).\n",
    "\n",
    "To simplify the creation of examples for such cases, LangChain provides a utility function `tool_example_to_messages` that generates a valid sequence for most model providers. It simplifies the generation of structured few-shot examples by just requiring Pydantic representations of the expected structured output:"
   ],
   "metadata": {
    "id": "Aj_eMALDr74Q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.utils.function_calling import tool_example_to_messages\n",
    "\n",
    "examples = [\n",
    "    (\n",
    "        \"cute!!!! the game is just to cute!!!! :3\",\n",
    "        AppReview(app_features=['cute game'], problems=[], sentiment=Sentiment.POSITIVE, dissatisfaction=1),\n",
    "    ),\n",
    "    (\n",
    "        \"amazing  well designed  but confusing. says i have a lot more free space than android reports.\",\n",
    "        AppReview(app_features=['well designed', 'amazing', 'confusing'], problems=['confusion about the available space'], sentiment=Sentiment.NEUTRAL, dissatisfaction=3),\n",
    "    ),\n",
    "    (\n",
    "        \"freezes every other spin crap game\",\n",
    "        AppReview(app_features=['crap game'], problems=['often freezes'], sentiment=Sentiment.NEGATIVE, dissatisfaction=5),\n",
    "    ),\n",
    "    (\n",
    "        \"3 stars very cute game but the ads is not cute:-):-)\",\n",
    "        AppReview(app_features=['very cute game', 'ads not cute'], problems=[], sentiment=Sentiment.NEUTRAL, dissatisfaction=2),\n",
    "    ),\n",
    "    (\n",
    "        \"stupid this is dumb.\",\n",
    "        AppReview(app_features=[], problems=[], sentiment=Sentiment.NEGATIVE, dissatisfaction=5),\n",
    "    ),\n",
    "    (\n",
    "        \"galaxy s2 love this app. very useful. only thing that bugs me is from time to time my screen blacks out and force closes the app when i back out.\",\n",
    "        AppReview(app_features=[\"very useful\"], problems=['screen blacks out', 'app forced to close'], sentiment=Sentiment.POSITIVE, dissatisfaction=2),\n",
    "    )\n",
    "]\n",
    "\n",
    "messages = []\n",
    "\n",
    "for ex_txt, ex_response in examples:\n",
    "    messages.extend(\n",
    "        tool_example_to_messages(ex_txt, [ex_response])\n",
    "    )"
   ],
   "metadata": {
    "id": "v27rUckUzf4n",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "96d00e34-b839-4302-bbf8-5c7035c2eaff"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check the generated messages:"
   ],
   "metadata": {
    "id": "RXJZXmF82HND"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for message in messages:\n",
    "    message.pretty_print()"
   ],
   "metadata": {
    "id": "RcVdi2kh2KyC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ea316010-6d28-4637-ab49-f05f8dc4a998"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can integrate the examples (represented as messages) in the prompt:"
   ],
   "metadata": {
    "id": "yTmii4QvN86j"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "examples_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            ie_system_msg + \"Please see below a few examples to help you with the information extraction task.\",\n",
    "        ),\n",
    "        MessagesPlaceholder('examples'), # a placeholder for passing in a list of messages\n",
    "        (\"human\", \"Extract details about a customer review of a mobile app, given below.\\n{text}\"),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "id": "ec9I4t0G2hpo"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "ie_examples_chain = examples_prompt_template | ie_llm\n",
    "\n",
    "for review in reviews:\n",
    "  # note that we are passing both messages that serve as examples and the review to be processed\n",
    "  annotation = ie_examples_chain.invoke({'examples':messages, 'text':review})\n",
    "  print(review)\n",
    "  print(annotation)\n",
    "  print()"
   ],
   "metadata": {
    "id": "prmuzayk_MT4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "155e8eb8-a56a-4903-b3e8-0f2d6f59d905"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combining two schemas\n",
    "\n",
    "We can define more than one schema and use them for extracting entites of different types - for example, about an event and people attending the event"
   ],
   "metadata": {
    "id": "dtenWlYPaR5M"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "\n",
    "class EventAttendee(BaseModel):\n",
    "    \"\"\"A person attending the event.\"\"\"\n",
    "\n",
    "    name: str = Field(\n",
    "        description=\"Name of the attendee.\")\n",
    "\n",
    "    role: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"The attendee's job role or title.\")\n",
    "\n",
    "\n",
    "class EventDetails(BaseModel):\n",
    "    \"\"\"Information about a specific event.\"\"\"\n",
    "\n",
    "    title: str = Field(description=\"The official title of the event.\")\n",
    "\n",
    "    date: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"The date of the event in YYYY-MM-DD format.\",\n",
    "        examples=['2025-11-10'])\n",
    "\n",
    "    location: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"The city and country where the event takes place.\")\n",
    "\n",
    "    attendees: List[EventAttendee] = Field(\n",
    "        default=[],\n",
    "        description=\"A list of key people attending the event.\")"
   ],
   "metadata": {
    "id": "xvQmuOYQaoaw"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "event_IE_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert information extraction assistant. Extract details about the event described below.\"),\n",
    "        (\"human\", \"Event information: {text_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "event_ie_llm = llm.with_structured_output(EventDetails)\n",
    "\n",
    "event_ie_chain = event_IE_prompt | event_ie_llm"
   ],
   "metadata": {
    "id": "UogsiPlTb41J"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "input_text = \"\"\"\n",
    "We are excited to announce the AI Frontier Summit, scheduled for 2026-03-15 in Tokyo, Japan.\n",
    "Key speakers include Dr. Emily Carter, the Chief Scientist, and Mr. Alex Chen, the Lead Engineer.\n",
    "\"\"\"\n",
    "\n",
    "extracted_data = event_ie_chain.invoke({\"text_input\": input_text})"
   ],
   "metadata": {
    "id": "dXVzv5TrcSDU"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "type(extracted_data)"
   ],
   "metadata": {
    "id": "ho9a3EatueUH",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "outputId": "2f6fd180-0664-4a46-80e2-7475d919bb87"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Extracted Title: {extracted_data.title}\")\n",
    "print(f\"Date: {extracted_data.date}\")\n",
    "print(\"Attendees:\")\n",
    "for attendie in extracted_data.attendees:\n",
    "  print(f\"\\t- {attendie.name}, {attendie.role}\")\n"
   ],
   "metadata": {
    "id": "aLHIqinrchCb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ddc38597-2e4e-46db-8293-07de035163ed"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Examining the IE chain on a larger data set\n"
   ],
   "metadata": {
    "id": "X1RzQ-6byyW7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To examine in more detail this kind of IE chain in the context of customer reviews we will use a data set of restaurant reviews that originates from Kaggle.com: [10000 Restaurant Reviews](https://www.kaggle.com/datasets/joebeachcapital/restaurant-reviews).<br>\n",
    "\n",
    "We will load a bunch of restaurant reviews, extract information about what customers liked and disliked, as well as their overall sentiment and compare the latter to human labels.\n",
    "\n",
    "First, load the data from the file"
   ],
   "metadata": {
    "id": "DKMfpEBsh5zh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "\n",
    "data_file = files.upload()"
   ],
   "metadata": {
    "id": "m4FwifhFiMKS",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "outputId": "1c2e4f04-d994-4573-fe0e-b32dd10abcca"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The uploaded object is a dictionary having the filename and content as itâ€™s key-value pair. So, we can extract the file name and read in the file content using pandas (to have it in the DataFrame format)"
   ],
   "metadata": {
    "id": "Fsi8pIuyiun2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_name = list(data_file.keys())[0]\n",
    "data = pd.read_csv(file_name)\n",
    "data.head(10)"
   ],
   "metadata": {
    "id": "BkHAKWFBio08",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "outputId": "a3b7b9a6-cffb-42b7-e5eb-77b47d266ebc"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "data.info()"
   ],
   "metadata": {
    "id": "V-CSmCiLkByu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9e12400f-d4c3-421a-ca0b-9e55782b14b8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# drop the columns we will not need\n",
    "data.drop(columns=['Pictures','7514','Metadata'], inplace=True)"
   ],
   "metadata": {
    "id": "UyDjFwO0Zkx7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# drop also rows without a review\n",
    "data.dropna(subset='Review', inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "id": "Keghr0thdfi2"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute and examine the review lenght as it does not make much sense to work with overly short reviews"
   ],
   "metadata": {
    "id": "-Drs-idSZ8kP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data['ReviewLen'] = data.Review.apply(len)\n",
    "data.ReviewLen.describe()"
   ],
   "metadata": {
    "id": "PRPsvtjqy6Cw",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "outputId": "ffd74dbb-8786-4189-aa90-4c7eef22ca4d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sb.kdeplot(data=data, x='ReviewLen', log_scale=True)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "k1G4LCVazxzR",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "outputId": "e841e160-f0cb-423e-b4a9-a4ded84d146b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Large majority of reviews are between 100 and 1000 characters long.\n",
    "\n",
    "We will exclude overly short reviews (typically not much can be extracted from them), by setting aside reviews with lenght below the 10th percentile."
   ],
   "metadata": {
    "id": "Xeyh9BX6aILA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "tenth_perc = np.percentile(data.ReviewLen.values, 10)\n",
    "# print(tenth_perc)\n",
    "\n",
    "data_sub = data.loc[data.ReviewLen > tenth_perc,].copy()\n",
    "data_sub.reset_index(drop=True, inplace=True)\n",
    "data_sub.info()"
   ],
   "metadata": {
    "id": "Id985nzO0MyU",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bc686259-c167-4239-a998-8202a8977ba1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# data_sub.ReviewLen.describe()"
   ],
   "metadata": {
    "id": "hcFyLeFduGET"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take a closer look at the reviews by taking a random sample from the dataset"
   ],
   "metadata": {
    "id": "_z8pkWdPqeo4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for review in data_sub.Review.sample(10, random_state=RAND_STATE):\n",
    "  print(review)\n",
    "  print(\"-------------------------------------------------------------------\")"
   ],
   "metadata": {
    "id": "lBOe5r2c10fU",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "991a97ce-c407-4e64-90ae-e9e8cfd40b3c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# make the review content more compact by eliminating multiple lines\n",
    "data_sub['Review'] = data_sub.Review.apply(lambda rev: rev.replace(\"\\n\", \" \"))"
   ],
   "metadata": {
    "id": "OaAFBOkJap2o"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add the 'Sentiment' column based on the customer ratings:"
   ],
   "metadata": {
    "id": "_BPy0-dheBPf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data_sub.Rating.value_counts()"
   ],
   "metadata": {
    "id": "QquBMdIa2yfW",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "outputId": "eace4c80-f7a7-4067-f2a5-090d823ec295"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "data_sub = data_sub.loc[data_sub.Rating != 'Like',]\n",
    "\n",
    "def sentiment(rating:float) -> str:\n",
    "  if float(rating) < 3: return \"negative\"\n",
    "  return \"positive\" if float(rating) > 3 else \"neutral\"\n",
    "\n",
    "data_sub['Rating2Sntmt'] = data_sub.Rating.apply(sentiment)\n",
    "data_sub.Rating2Sntmt.value_counts()"
   ],
   "metadata": {
    "id": "1SfJbKv0RsF3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "outputId": "2498302d-6c47-4cc3-9b9d-17a8805da17f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, define the schema for IE from restaurant reviews"
   ],
   "metadata": {
    "id": "dHU1oMtAeMAc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class RestaurantReview(BaseModel):\n",
    "    \"\"\"Information about customer opinions and sentiment expressed in a restaurant review.\"\"\"\n",
    "\n",
    "    sentiment: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"The overall sentiment expressed in the review. Possible options are: positive, negative, neutral\",\n",
    "        examples=['positive']\n",
    "    )\n",
    "\n",
    "    food_positives: Optional[list] = Field(\n",
    "        default=[],\n",
    "        description=\"Positive opinion(s) and satisfaction expressed in the review about the food served in the restaurant. This field is about food only and no other aspects of the restaurant.\",\n",
    "        examples=[[\"amazing punjabi food\"]]\n",
    "    )\n",
    "\n",
    "    other_positives: Optional[list] = Field(\n",
    "        default=[],\n",
    "        description=\"Positive opinion(s) and satisfaction expressed in the review about the restaurant itself, including, for example, its facilites, staff, and atmosphere. This field is about the customer's positive perceptions of various aspects of the restaurant EXCEPT food.\",\n",
    "        examples=[[\"service was quick\", \"staff did their best\"]]\n",
    "    )\n",
    "\n",
    "    food_negatives: Optional[list] = Field(\n",
    "        default=[],\n",
    "        description=\"Negative opinion(s) and dissatisfaction expressed in the review about the food served in the restaurant. This field is about food only and no other aspects of the restaurant.\",\n",
    "        examples=[[\"stale chicken\", \"cold roties\"]]\n",
    "    )\n",
    "\n",
    "    other_negatives: Optional[list] = Field(\n",
    "        default=[],\n",
    "        description=\"Negative opinion(s) and dissatisfaction expressed in the review about the restaurant itself, including, for example, its facilites, staff, and atmosphere. This field is about the customer's negative perceceptions about various aspects of the restaurant EXCEPT food.\",\n",
    "        examples=[[\"long waiting to be served\"]]\n",
    "    )\n",
    "\n"
   ],
   "metadata": {
    "id": "hHfYlzD0TRXY"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use the same IE prompt template as the one above (`ie_prompt_template`), but we need to instiate an LLM with a different IE schema:"
   ],
   "metadata": {
    "id": "HWfQOVB2VYOI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "llm = ChatGroq(model=model_name, temperature=0.1)\n",
    "restaurant_ie_llm = llm.with_structured_output(schema=RestaurantReview)\n",
    "\n",
    "\n",
    "restaurant_ie_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", ie_system_msg),\n",
    "        (\"human\", \"Extract details about a restaurant review, given below:\\n{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "restaurant_ie_chain = restaurant_ie_prompt_template | restaurant_ie_llm"
   ],
   "metadata": {
    "id": "r2g3uOenXAFx"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, try the IE chain on a few reviews only"
   ],
   "metadata": {
    "id": "ULib_jnyf5qo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import textwrap\n",
    "\n",
    "restaturant_reviews_sample = data_sub.Review.sample(3, random_state=RAND_STATE).to_list()\n",
    "for review in restaturant_reviews_sample:\n",
    "  response = restaurant_ie_chain.invoke({'text': review})\n",
    "  print(textwrap.fill(review, width=100))\n",
    "  print()\n",
    "  print(\"Extracted info:\")\n",
    "  print(f\"Overall sentiment: {response.sentiment}\")\n",
    "  print(f\"Positive food-related comments: {response.food_positives}\")\n",
    "  print(f\"Negative food-related comments: {response.food_negatives}\")\n",
    "  print(f\"Other positive comments: {response.other_positives}\")\n",
    "  print(f\"Other negative comments: {response.other_negatives}\")\n",
    "  print(\"-----------------------------------------------------------------------\")"
   ],
   "metadata": {
    "id": "hOp1md3zVtYo",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "74d3789e-432f-4b8e-bbd2-7ee6aa09c0bd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Considering the restrictions on the Groq API use, instead of tagging all reviews in our data subset, we will just demonstrate the process on a small sample.\n",
    "Since the dataset is biased towards positive reviews, to explore how the model is able to deal with all three kinds of sentiment, we will randomly take 20 reviews from each sentiment category:"
   ],
   "metadata": {
    "id": "tWun-9IOj3Lk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sample_size = 20\n",
    "rand_state = RAND_STATE\n",
    "\n",
    "pos_sample = data_sub.loc[data_sub.Rating2Sntmt=='positive'].sample(sample_size, random_state=rand_state)\n",
    "neg_sample = data_sub.loc[data_sub.Rating2Sntmt=='negative'].sample(sample_size, random_state=rand_state)\n",
    "neutral_sample = data_sub.loc[data_sub.Rating2Sntmt=='neutral'].sample(sample_size, random_state=rand_state)\n",
    "\n",
    "review_sample = pd.concat([pos_sample, neg_sample, neutral_sample], axis=0, ignore_index=True)\n",
    "review_sample = review_sample.sample(frac=1, ignore_index=True) # shuffle all rows\n",
    "review_sample.head()"
   ],
   "metadata": {
    "id": "22u2c2UImZEi",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "outputId": "3f651867-35df-4778-af3a-3d3c7fd42086"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can pass each review to the IE chain and structure the results into a data frame for easier later integration with the reviews samples"
   ],
   "metadata": {
    "id": "M0Lak432smZa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from time import sleep\n",
    "\n",
    "def llm_tagging(review:str) -> RestaurantReview:\n",
    "  try:\n",
    "    return restaurant_ie_chain.invoke({\"text\":review})\n",
    "  except Exception as e:\n",
    "    print(f\"Error when processing review \\\"{review}\\\":\\n{e}\")\n",
    "    return None\n",
    "  finally:\n",
    "    sleep(3)\n",
    "\n",
    "\n",
    "tagging_results = []\n",
    "for i, review in enumerate(review_sample.Review.tolist()):\n",
    "\n",
    "  if i % 10 == 0: print(f\"Processing {i+1}. review\")\n",
    "\n",
    "  res = llm_tagging(review)\n",
    "  tagging_results.append({\n",
    "      'sentiment': res.sentiment if res else None,\n",
    "      'food_positives': res.food_positives if res else None,\n",
    "      'food_negatives': res.food_negatives if res else None,\n",
    "      'other_positives': res.other_positives if res else None,\n",
    "      'other_negativess': res.other_negatives if res else None\n",
    "  })\n",
    "\n",
    "\n",
    "tagging_res_df = pd.DataFrame(tagging_results)"
   ],
   "metadata": {
    "id": "_93MotQBtFVn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cd75e6f5-a838-4ed6-9530-621e87222fe0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "tagging_res_df.head(10)"
   ],
   "metadata": {
    "id": "2eH1DcWrqPYL",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "outputId": "79ecf705-a5fa-41db-a37b-e1c046bff814"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "tagged_review_sample = pd.concat([review_sample, tagging_res_df], axis=1)\n",
    "tagged_review_sample.head()"
   ],
   "metadata": {
    "id": "n93JQvnbw2F3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 643
    },
    "outputId": "45a0a19a-bf8b-49a2-a770-da61701551a4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check the level of matching between human labels and the labels assigned by the LLM:"
   ],
   "metadata": {
    "id": "LrKUfk2cXHhV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from statistics import mean\n",
    "\n",
    "def label_match(row):\n",
    "  human_lbl = row['Rating2Sntmt']\n",
    "  llm_lbl = \"other\" if not row['sentiment'] else row['sentiment']\n",
    "  return human_lbl == llm_lbl\n",
    "\n",
    "sentiment_match = tagged_review_sample.apply(label_match, axis=1)\n",
    "print(f\"Number of matched labels: {sum(sentiment_match)}\")\n",
    "print(f\"Proportion of matched labels: {mean(sentiment_match):.4f}\")"
   ],
   "metadata": {
    "id": "PR02k1jfXV79",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5c309237-8a97-462f-f10b-68123132ae5c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We should explore where mismatches are and try to understand why the tagging was wrongly done"
   ],
   "metadata": {
    "id": "sAOvaWvWZqJ6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tagged_review_sample['sentiment_matched'] = sentiment_match\n",
    "tagged_review_sample.loc[~tagged_review_sample.sentiment_matched, ['Review', 'Rating2Sntmt', 'sentiment']]"
   ],
   "metadata": {
    "id": "emYRSCFEZz8v",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "outputId": "08157da4-7017-4531-83bf-9cbf1fd8a5e5"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mixed results - for some reviews, the LLM was clearly wrong, but there are also cases where the sentiment it assigned looks more appropriate than the one determined by the customer scores"
   ],
   "metadata": {
    "id": "WsumUPb2neKG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tagged_review_sample.sentiment.value_counts()"
   ],
   "metadata": {
    "id": "n53GwpQbnLtG",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "outputId": "c6fe4e88-c146-4a72-d309-9ee102ca2be2"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems that the model has a bias towards negative sentiment."
   ],
   "metadata": {
    "id": "SPvIJeB7aUt2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### To explore further:\n",
    "\n",
    "#### Knowedge graph extraction\n",
    "\n",
    "An interesting thing to explore further is the use of LangChain for the extraction of a knowledge graph from the given textual content. This is a more advanced form of information extraction since it includes extracting entities mentioned in the text and their relationships, followed by the creation of a graph out of the extracted information (nodes are the identified entities, whereas edges are the relationships identified among the entities).\n",
    "\n",
    "For an example how it can be done, check the code and the tutorial at this GitHub repo: [https://github.com/thu-vu92/knowledge-graph-llms/](https://github.com/thu-vu92/knowledge-graph-llms/)\n",
    "\n",
    "#### LangExtract\n",
    "\n",
    "In addition, Google's [LangExtract](https://github.com/google/langextract) library is anther LLM-based option for IE extraction, which seems to be more promising (that is, allow for more precise IE) than the approach presented here."
   ],
   "metadata": {
    "id": "Wy2_6P0OYvfX"
   }
  }
 ]
}
